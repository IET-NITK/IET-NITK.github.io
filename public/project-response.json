{
  "meta": {
    "total_count": 5
  },
  "items": [
    {
      "id": 12,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/12/",
        "html_url": "http://localhost/projects/audio-fingerprinting/",
        "slug": "audio-fingerprinting",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-01-29T15:27:29.458646Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Audio Fingerprinting",
      "project_title": "Audio Fingerprinting",
      "project_sig": "CIPHER",
      "project_slug": "audiofingerprinting",
      "project_authors": "Attada Ramprasad, Pooja Gayathri Kanala, Sharuf Baig, K Snehith Bhagavan",
      "github_url": "https://github.com/IET-NITK/AudioFingerprinting",
      "project_img_url": "https://tetragram.codered.cloud/media/images/1_smebNTc.max-800x600.png",
      "project_description": "Recognizing a song from a large cluster of audio can't be achieved by using brute force to compare an audio sample to every song in the database. In this project we use hashing,a process in which reproducible hash tokens are extracted to save the effort.",
      "project_tags": "Audio, Python, Maths",
      "project_body": "# Audio Fingerprinting\r\n\r\n\r\n## ABSTRACT\r\nRecognizing a song from a large cluster of audio can't be achieved by using brute force to compare an audio sample to every song in the database. In this project we use hashing,a process in which reproducible hash tokens are extracted to save the effort, this method compares the hash values instead of the whole ï¬les so that it will be more efficient, Audio fingerprinting is the process of representing an audio signal in a compact way by extracting relevant features of the audio content.The fingerprints from the unknown sample are matched against a large set of fingerprints stored in the database. In this our model we are using mysql as the database.Companies like shazam, phillips, intrasonics and many more use audio fingerprinting for various implementations.\r\n\r\n## AUDIO RECOGNITION\r\nRead the audio file recorded and store it into a 2-D array of amplitude against time.The recording can be done using the microphone or the input file can be read from the system itself and saved as an audio file.This process is same for both , adding the audio files to the database as well as for recognising the audio files i.e both training and testing phase.\r\n\r\n![audio](https://tetragram.codered.cloud/media/images/1_smebNTc.max-800x600.png)\r\n\r\n## Fast Fourier Transform (FFT)\r\nNow we use Fast Fourier Transform (FFT) to change the waveform to frequency domain from time domain.\r\n\r\n![audio](https://tetragram.codered.cloud/media/images/2_aRt4ylH.max-800x600.png)\r\n\r\n## Spectrogram\r\nNext step is to perform Short-Term Fourier Transform (STFT) of the audio signal by breaking down the signal into small chunks and performing the Fourier Transform on each of them to generate the spectrogram, which is a visual plot of all three variables amplitude against time and frequency.\r\n\r\n![audio](https://tetragram.codered.cloud/media/images/3_sZdzF6w.max-800x600.png)\r\n\r\n## Mapping peaks\r\nThe processing is usually carried out on a 2-D array , which stores the STFT coefficients of the file and the peaks, local maxima points of the file, which are mapped by masking.A set of the peak and its neighbour are passed to a hash function to generate a hash. A hash is an encoded string which is unique for each input. An audio fingerprint is generated which is a set of hash values and the offset value(time component of the peak).This value is stored in the database with a unique song_id. After we perform these steps on the known file we can match the audio file.\r\n\r\n## Recognise the song\r\nWe recognise the song by comparing the hash value from the database.A pair of key-value is appended into an empty dictionary created for each song. Where key is the difference between the database offset and the sample offset and value if the number of repetitions of the matches we get while comparing the hash values. A score is calculated for each song which is the maximum value of â€˜valueâ€™ in that particular dictionary. The song with maximum score is the best match for the input file and the model returns the song name with the score value.\r\n\r\n## Conclusion\r\nThe model has been successful in recognizing the song by finding the fingerprints. The further improvements for this model would be to use noise filter techniques, get accurate fingerprints and improve the database.\r\n\r\n## RESOURCES\r\n- [medium article on audio fingerprinting](https://medium.com/swlh/understanding-audio-fingerprinting-b39682aa3b5f)\r\n- [ourcodeworld.com/shazamclone](https://ourcodeworld.com/articles/read/973/creating-your-own-shazam-identify-songs-with-python-through-audio-fingerprinting-in-ubuntu-18-04)"
    },
    {
      "id": 13,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/13/",
        "html_url": "http://localhost/projects/audio-to-sign-language-translator/",
        "slug": "audio-to-sign-language-translator",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-01-29T15:28:26.615757Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Audio to Sign Language Translator",
      "project_title": "Audio to Sign Language Translator",
      "project_sig": "CIPHER",
      "project_slug": "audiotosignlanguagetranslator",
      "project_authors": "Jagan Babu Korra, Vaishnavi Prakash Kalgutkar, K Sriram, Kowshic V, Ansh Bindlish",
      "github_url": "https://github.com/IET-NITK/A2SL-Translator",
      "project_img_url": "https://user-images.githubusercontent.com/76841315/162028693-7aa2e6af-3a7f-4f07-a088-e30628ec754b.png",
      "project_description": "Sign languages are visual languages that use hand, facial and body movements as a means of communication. There are over 135 different sign languages all around the world including American Sign Language (ASL), Indian Sign Language (ISL) and British Sign Language (BSL).",
      "project_tags": "ASL, Audio, Python",
      "project_body": "## Methodology\r\n\r\n- Audio input on the platform using Python Speech Recognition module.\r\n- Conversion of audio to text using Google Speech API.\r\n- Dependency parser for analysing grammatical structure of the sentence and establishing relationship between words.\r\n- ISL Generator: ISL of input sentence using ISL grammar rules.\r\n- Generation of Sign language with concatenating videos\r\n\r\n## Libraries used\r\n- streamlit\r\n- speech_recognition\r\n- cv2\r\n- numpy\r\n- moviepy\r\n- nltk\r\n\r\n## Timeline\r\n\r\n| **STEP** | **TIME NEEDED** |\r\n| --- | ----------- |\r\n| Learning process | 35 days |\r\n| Working on theory to design | 15 days |\r\n| Designing the model/Coding | 25 days |\r\n| Working on changes and simulations\t | 10 days |\r\n| Analysis | 2 days |\r\n\r\n## Challenges\r\n- Compiling a quality repository of words and alphabets for output video.\r\n- Time required in processing video."
    },
    {
      "id": 23,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/23/",
        "html_url": "http://localhost/projects/handwritten-equation-solver/",
        "slug": "handwritten-equation-solver",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-02-28T16:01:50.636454Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Handwritten Equation Solver",
      "project_title": "Handwritten Equation Solver",
      "project_sig": "CIPHER",
      "project_slug": "equationsolver",
      "project_authors": "Rachana P J, Dolly Gupta, M B Sai Aditya, Lalit Nagar",
      "github_url": "https://github.com/IET-NITK/HandwrittenEquationSolver",
      "project_img_url": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTbMjcHRruOCt6rufgfSArkSHIIqbxPz1tcLN4gUVXixlqLYkpEVJqFq2GgARkKRzl0aV0&usqp=CAU",
      "project_description": "As the name of the project suggests, we intend to make such a system(Web Interface) in which user will just upload his/her handwritten equation(s) and in return would get the solution to that.",
      "project_tags": "Python, Deep Learning, MBSA",
      "project_body": "# Handwritten Equation Solver\r\n\r\n\r\n## AIM of the Project\r\nAs the name of the project suggests, we intend to make such a system(Web Interface) in which user will just upload his/her handwritten equation(s) and in return would get the solution to that.\r\n> Note: The project is restricted only for Polynomials & Linear Equations(1 & 2 Variable)\r\n\r\n## Implementation\r\n- For detecting handwritten equation, we need to detect number,mathematical symbols,variables etc. So firstly, we trained a CNN(Convolutional Nueral Network) Model over a specified dataset.\r\n- Now once the CNN Model was tested enough for detecting handwritten contents, we further proceed to apply some Algebra to solve these detected Equations/Polynomials.\r\n- Then, we created a Frontend where user can upload & crop the images of the handwritten equation and can feed it further to the Backend Servers.\r\n- The user uploaded images are way to diffrent from the images that CNN demands,Hence we applied Image processing(OpenCV) on the user uploaded image to convert it into a desired image.\r\n\r\n## Frameworks & Modules Used\r\n- **Tensorflow** & **Keras** (for Training & Testing CNN Model)\r\n- **Flask** Web Framework (for Backend)\r\n- **HTML**,**CSS**,**JS**,**Bootstrap**(for Frontend)\r\n- **OpenCV** Library of Python (for Image Processing)\r\n- **sympy** module(Handles all equation Solving)\r\n\r\n## Working of the Project\r\n> Note: For the project 'a','b' are variables & 'x' is the multiplication sign.Also '=0' is already understood by the model so user need not write it. So if the Equation is 15a=45, then user need to write 15a-45 and then upload it on the website.\r\n- The user uploads an image to the website as shown below:-\r\n![eq](https://user-images.githubusercontent.com/85332648/161968276-040dad35-b81d-4a47-b595-f428496ff800.jpeg)\r\n- Then the image is processed and converted to desired image that model wants as shown below.\r\n![image](https://user-images.githubusercontent.com/85332648/161968517-78d5adaf-96bc-472b-a5c2-7dbab94c7bdf.png)\r\n- Model then detect that an equation with **variable 'a'** has been uploaded & generates the following output.\r\n![image](https://user-images.githubusercontent.com/85332648/161969317-027e503f-a9a7-45b4-b077-d06458b8e9b0.png)\r\n- Similarly for **Linear Equation in two variable** user need to upload two Images as shown below:-\r\n![eq1](https://user-images.githubusercontent.com/85332648/161969873-2dd69652-67b2-4930-9de3-c97184875e3c.jpeg)\r\n![eq2](https://user-images.githubusercontent.com/85332648/161969912-505cc709-95ba-492b-bfd7-dcce54944027.jpeg)\r\n- Again Model coverts both images as desired images & Solves it!!\r\n![image](https://user-images.githubusercontent.com/85332648/161970223-07b7aaa5-ef10-49d9-b28e-d6ff49df8766.png)\r\n![image](https://user-images.githubusercontent.com/85332648/161970272-64c63894-d011-4e15-9264-5f33c4609531.png)\r\n- **Model produces Following output:-**\r\n\r\n![image](https://user-images.githubusercontent.com/85332648/161970375-8d5d9888-e699-4a15-819d-52a759ddd2cc.png)\r\n\r\n## Why CNN?\r\n- In traditional Neural Networks, one need to specify the important features to be considered while **CNN** automatically detects the important features without any **Human Supervision.**\r\n- Our project involved **high level pixel processing** and CNN's are considered to be the best for that as it can learn the key features for any class itself.\r\n- **CNN's** even have better accuracy than any other **Machine Learning Model** when it comes to **\"Detection\"**.Hence CNN was the best thing to be used for this project.\r\n\r\n## Challenges We ran into\r\n- When we trained the CNN Model for **\"=\"** , **\"(\"** & **\")\"** signs, its accuracy reduced significantly which is absurd.\r\n- Introducing variable **\"x\"** was challenging as **\"x\"** also represents multiplication sign. So we introduced **\"a\"** & **\"b\"** as variables.\r\n- User is constrained to write the equation in a paper with white unruled background. Ruled background confuses the CNN Model & it produces unusual results.\r\n\r\n## Steps to Test Backend\r\n- Download the **.ipynb** file from the github & upload it in **Google Colab**.\r\n- Also Do upload the **model** folder in the colab environment.\r\n- Now you may upload your own custom equations in the Colab environment & all other steps are mentioned in the .ipynb file itself.\r\n> Do Ensure that you crop the equation to the maximum extent.\r\n> Do this!!!ðŸ‘‡ðŸ‘‡ðŸ‘‡\r\n\r\n![2](https://user-images.githubusercontent.com/85332648/161974694-5f5fb2a0-1244-47f1-b9c5-3164d41d216c.jpeg)\r\n> Not this!!!ðŸ‘‡ðŸ‘‡ðŸ‘‡\r\n\r\n![eq7](https://user-images.githubusercontent.com/85332648/161974775-dd58c278-7488-45ca-b272-f08e15a5ce46.jpeg)"
    },
    {
      "id": 24,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/24/",
        "html_url": "http://localhost/projects/sudoku-solver/",
        "slug": "sudoku-solver",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-02-28T16:04:35.812062Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Sudoku Solver",
      "project_title": "Sudoku Solver",
      "project_sig": "CIPHER",
      "project_slug": "sudokusolver",
      "project_authors": "Monika Agarwal, Verupaka Yashwanth Reddy, Adith Shekhar Gatty, Raunak Somani, Pragya Kiran",
      "github_url": "https://github.com/IET-NITK/SudokuSolver",
      "project_img_url": "https://images.unsplash.com/photo-1640537702474-3e503c21eefc?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D",
      "project_description": "Sudoku is a logic-based puzzle where the user is given a 9 x 9 grid, which consists of nine 3 x 3 sub-grids, and the objective of the puzzle is to fill these sub-grids with numbers from 1 to 9 such that no two numbers appear in the same row, same column and the same sub-grid of the 9 x 9 grid.",
      "project_tags": "Sudoku, Logic, Maths",
      "project_body": "# Sudoku Solver\r\n\r\n## Introduction\r\nSudoku is a logic-based puzzle where the user is given a 9 x 9 grid, which consists of nine 3 x 3 sub-grids, and the objective of the puzzle is to fill these sub-grids with numbers from 1 to 9 such that no two numbers appear in the same row, same column and the same sub-grid of the 9 x 9 grid. Initially, the grid is partially filled with digits in specific locations to make sure the puzzle is well-posed and has a single solution. This is a very popular puzzle that has been featured in newspapers since the 19th century, the first being featured in a French newspaper. The puzzle can range in difficulty from easy to hard and is also quite addictive. There are many people who start off their typical day by reading the newspaper in the morning and solving puzzles like Sudoku. The Sudoku solver application aids the users to obtain a solution of a sudoku puzzle by providing an incomplete sudoku puzzle as the input. The further sections will cover the functionality and the working of the Sudoku solver application.\r\n\r\n## Functionality of the Application\r\nThe Sudoku solver application takes the incomplete Sudoku as the input and, if valid, solves the Sudoku and displays it; otherwise displays the error message indicating invalid input.\r\nThe application enables the user to provide input in two ways:\r\n\r\n- **Camera/Photo upload:** User has the option to click or the upload the photo of the sudoku as the input\r\n- **Manual input:** User needs to manually enter the numbers in the corresponding boxes as the input. If the given input is solvable or valid and the application solves and displays the complete sudoku. Otherwise it will give out a message indicating that the sudoku was invalid i.e there was no solution possible in such a way that it follows all the rules of a classic sudoku puzzle.\r\n\r\n## Working of the Application\r\nThe basic user interface of the application was developed using Flutter, which is a software development kit created by Google and uses the Dart programming language to develop applications. This app consists of 2 tabs and the user between these tabs (by default the first tab remains activated) by clicking the respective button in the bottom navigation bar. The two tabs are:\r\n\r\n1. **Scan/Upload tab:**\r\nThis tab enables the user to provide an input via an image. They need to click the camera icon to take a photo and click the image icon to upload a photo. Once the required photo is clicked or uploaded, they are given an option to crop the image and they must crop out only the sudoku puzzle out of that image(Important). The cropped out sudoku puzzle is then displayed on the screen. This feature of providing an image as the input by taking a picture or uploading an image and then cropping out the image was made possible by using the **document_scanner_flutter 0.2.5 **dart package. The user must click the â€˜OKâ€™ button below the displayed image to start the text scanning process. For this functionality **google_ml_vision 0.0.7 **dart package was used which returned all the scanned text and its corresponding bounding box (coordinates of the box around the text in pixels) from the image. With this, their respective index is calculated and filled in the 9 x 9 matrix. After the scanning process the application directs the user to the second tab.\r\n2. **Fill tab:**\r\nThis tab initially displays an empty sudoku i.e 9 x 9 boxes or cells with no values. The user has the option to enter a value in the cell by clicking the particular cell (which highlights the cell) and then clicking the button with their required number. There are also options to clear a selected cell or reset the whole sudoku. Note: Changes in the sudoku cause the corresponding changes in the 9 x 9 matrix that was initially declared as a zero 9 x 9 matrix (where 0 indicates no value at that particular position). If the user has given the input through 1st Tab, they are directed to the second tab and the corresponding changes in the 9 x 9 matrix is reflected in the displayed sudoku. The user can make some changes to make sure it matches the input sudoku in case of some errors. Note: The result from scanning isnâ€™t always accurate and might miss some numbers or display wrong numbers in the wrong position. Finally the user must click the submit button which sends the 9 x 9 matrix to a function to implement the sudoku solving algorithm.\r\n\r\n**Sudoku solving algorithm:**\r\nThe main algorithm used in the process is **Backtracking**. we have made some sub functions inside the code to solve the different part of algorithm for sudoku solver including :\r\n\r\n**IsValid():** This function is basically made for checking the validity of a particular number in a specific index of 9 X 9 grid for correct sudoku. It takes input as grid, number, rowindex, columnindex and gives boolean validation.\r\n\r\n**SolveSudoku():** This function is basically made for solving 9 X 9 grid into correct sudoku. It terminates when the index reaches the end of the grid and also handles the boundary conditions.\r\nThe final output will depend on the return statement where true will display the matrix in the form of sudoku and false will display a message indicating invalid input.\r\n\r\n## Dart packages used in the application:\r\n\r\n- [google_ml_vision(v0.0.7)](https://pub.dev/packages/google_ml_vision)\r\n- [document_scanner_flutter(v0.2.5)](https://pub.dev/packages/document_scanner)"
    },
    {
      "id": 25,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/25/",
        "html_url": "http://localhost/projects/youtube-transcript-summarizer/",
        "slug": "youtube-transcript-summarizer",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-02-28T16:07:04.847112Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "YouTube Transcript Summarizer",
      "project_title": "YouTube Transcript Summarizer",
      "project_sig": "CIPHER",
      "project_slug": "youtubetranscriptsummarizer",
      "project_authors": "Suyash Satish Chintawar, Feyaz Baker, Venkat Rohit Merugu, Naveen Kumar, Gaurav Kumar, Sahana",
      "github_url": "https://github.com/IET-NITK/YT-Transcript-Summarizer",
      "project_img_url": "https://images.unsplash.com/photo-1521302200778-33500795e128?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D",
      "project_description": "Throughout the day, an enormous number of video recordings are generated and shared on the Internet. It has become quite difficult to devote time to watching such videos, which may last longer than expected, and our efforts may be in vain if we are unable to extract useful information from them.",
      "project_tags": "Youtube",
      "project_body": "# YouTube Transcript Summarizer\r\n\r\n\r\n## Introduction\r\nThroughout the day, an enormous number of video recordings are generated and shared on the Internet. It has become quite difficult to devote time to watching such videos, which may last longer than expected, and our efforts may be in vain if we are unable to extract useful information from them.\r\n\r\nSummarizing transcripts of such videos allows us to rapidly look for relevant patterns in the video and saves us time and effort from having to go through the entire content. This project will allow us to gain hands-on experience with cutting-edge NLP techniques for abstractive text summarization.\r\n\r\n## Dependencies and Requirements\r\n- `flask==2.0.2`\r\n- `torch==1.10.2`\r\n- `urllib3==1.26.8`\r\n- `transformers==4.16.2`\r\n- `huggingface-hub==0.4.0`\r\n- `youtube-transcript-api==0.4.3`\r\n- `bert-extractive-summarizer==0.10.1`\r\n\r\n## Running the website\r\n1. Clone repository\r\n\r\n`git clone https://github.com/IET-NITK/IETC-YT-Transcript-Summarizer`\r\n\r\n2. Install requirements\r\n\r\n`pip install -r requirements.txt`\r\n\r\n3. Run website\r\n\r\n`python3 app.py`\r\n\r\n## Methodology\r\nThree major steps are involved:\r\n\r\n- Fetching Subtitles : â€˜youtube_transcript_apiâ€™ has been used to extract summary for a youtube video by passing the video ID as the input to the api.\r\n- Summarizing transcripts : BERT extractive summarisation has been used to summarize the obtained video transcript.\r\n- Display summary on website : The summary is then displayed on the website hosted on flask server.\r\n\r\n## Challenges\r\n- Finding good abstractive text summarization techniques.\r\n- Time required in summarization."
    }
  ]
}