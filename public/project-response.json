{
  "meta": {
    "total_count": 14
  },
  "items": [
    {
      "id": 12,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/12/",
        "html_url": "http://localhost/projects/audio-fingerprinting/",
        "slug": "audio-fingerprinting",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-01-29T15:27:29.458646Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Audio Fingerprinting",
      "project_title": "Audio Fingerprinting",
      "project_sig": "CIPHER",
      "project_slug": "audiofingerprinting",
      "project_year": 2023,
      "project_authors": "Attada Ramprasad, Pooja Gayathri Kanala, Sharuf Baig, K Snehith Bhagavan",
      "github_url": "https://github.com/IET-NITK/AudioFingerprinting",
      "project_img_url": "https://tetragram.codered.cloud/media/images/1_smebNTc.max-800x600.png",
      "project_description": "Recognizing a song from a large cluster of audio can't be achieved by using brute force to compare an audio sample to every song in the database. In this project we use hashing,a process in which reproducible hash tokens are extracted to save the effort.",
      "project_tags": "Audio, Python, Maths",
      "project_body": "# Audio Fingerprinting\r\n\r\n\r\n## ABSTRACT\r\nRecognizing a song from a large cluster of audio can't be achieved by using brute force to compare an audio sample to every song in the database. In this project we use hashing,a process in which reproducible hash tokens are extracted to save the effort, this method compares the hash values instead of the whole Ô¨Åles so that it will be more efficient, Audio fingerprinting is the process of representing an audio signal in a compact way by extracting relevant features of the audio content.The fingerprints from the unknown sample are matched against a large set of fingerprints stored in the database. In this our model we are using mysql as the database.Companies like shazam, phillips, intrasonics and many more use audio fingerprinting for various implementations.\r\n\r\n## AUDIO RECOGNITION\r\nRead the audio file recorded and store it into a 2-D array of amplitude against time.The recording can be done using the microphone or the input file can be read from the system itself and saved as an audio file.This process is same for both , adding the audio files to the database as well as for recognising the audio files i.e both training and testing phase.\r\n\r\n![audio](https://tetragram.codered.cloud/media/images/1_smebNTc.max-800x600.png)\r\n\r\n## Fast Fourier Transform (FFT)\r\nNow we use Fast Fourier Transform (FFT) to change the waveform to frequency domain from time domain.\r\n\r\n![audio](https://tetragram.codered.cloud/media/images/2_aRt4ylH.max-800x600.png)\r\n\r\n## Spectrogram\r\nNext step is to perform Short-Term Fourier Transform (STFT) of the audio signal by breaking down the signal into small chunks and performing the Fourier Transform on each of them to generate the spectrogram, which is a visual plot of all three variables amplitude against time and frequency.\r\n\r\n![audio](https://tetragram.codered.cloud/media/images/3_sZdzF6w.max-800x600.png)\r\n\r\n## Mapping peaks\r\nThe processing is usually carried out on a 2-D array , which stores the STFT coefficients of the file and the peaks, local maxima points of the file, which are mapped by masking.A set of the peak and its neighbour are passed to a hash function to generate a hash. A hash is an encoded string which is unique for each input. An audio fingerprint is generated which is a set of hash values and the offset value(time component of the peak).This value is stored in the database with a unique song_id. After we perform these steps on the known file we can match the audio file.\r\n\r\n## Recognise the song\r\nWe recognise the song by comparing the hash value from the database.A pair of key-value is appended into an empty dictionary created for each song. Where key is the difference between the database offset and the sample offset and value if the number of repetitions of the matches we get while comparing the hash values. A score is calculated for each song which is the maximum value of ‚Äòvalue‚Äô in that particular dictionary. The song with maximum score is the best match for the input file and the model returns the song name with the score value.\r\n\r\n## Conclusion\r\nThe model has been successful in recognizing the song by finding the fingerprints. The further improvements for this model would be to use noise filter techniques, get accurate fingerprints and improve the database.\r\n\r\n## RESOURCES\r\n- [medium article on audio fingerprinting](https://medium.com/swlh/understanding-audio-fingerprinting-b39682aa3b5f)\r\n- [ourcodeworld.com/shazamclone](https://ourcodeworld.com/articles/read/973/creating-your-own-shazam-identify-songs-with-python-through-audio-fingerprinting-in-ubuntu-18-04)"
    },
    {
      "id": 13,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/13/",
        "html_url": "http://localhost/projects/audio-to-sign-language-translator/",
        "slug": "audio-to-sign-language-translator",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-01-29T15:28:26.615757Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Audio to Sign Language Translator",
      "project_title": "Audio to Sign Language Translator",
      "project_sig": "CIPHER",
      "project_slug": "audiotosignlanguagetranslator",
      "project_year": 2022,
      "project_authors": "Jagan Babu Korra, Vaishnavi Prakash Kalgutkar, K Sriram, Kowshic V, Ansh Bindlish",
      "github_url": "https://github.com/IET-NITK/A2SL-Translator",
      "project_img_url": "https://user-images.githubusercontent.com/76841315/162028693-7aa2e6af-3a7f-4f07-a088-e30628ec754b.png",
      "project_description": "Sign languages are visual languages that use hand, facial and body movements as a means of communication. There are over 135 different sign languages all around the world including American Sign Language (ASL), Indian Sign Language (ISL) and British Sign Language (BSL).",
      "project_tags": "ASL, Audio, Python",
      "project_body": "## Methodology\r\n\r\n- Audio input on the platform using Python Speech Recognition module.\r\n- Conversion of audio to text using Google Speech API.\r\n- Dependency parser for analysing grammatical structure of the sentence and establishing relationship between words.\r\n- ISL Generator: ISL of input sentence using ISL grammar rules.\r\n- Generation of Sign language with concatenating videos\r\n\r\n## Libraries used\r\n- streamlit\r\n- speech_recognition\r\n- cv2\r\n- numpy\r\n- moviepy\r\n- nltk\r\n\r\n## Timeline\r\n\r\n| **STEP** | **TIME NEEDED** |\r\n| --- | ----------- |\r\n| Learning process | 35 days |\r\n| Working on theory to design | 15 days |\r\n| Designing the model/Coding | 25 days |\r\n| Working on changes and simulations\t | 10 days |\r\n| Analysis | 2 days |\r\n\r\n## Challenges\r\n- Compiling a quality repository of words and alphabets for output video.\r\n- Time required in processing video."
    },
    {
      "id": 23,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/23/",
        "html_url": "http://localhost/projects/handwritten-equation-solver/",
        "slug": "handwritten-equation-solver",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-02-28T16:01:50.636454Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Handwritten Equation Solver",
      "project_title": "Handwritten Equation Solver",
      "project_sig": "CIPHER",
      "project_slug": "equationsolver",
      "project_year": 2022,
      "project_authors": "Rachana P J, Dolly Gupta, M B Sai Aditya, Lalit Nagar",
      "github_url": "https://github.com/IET-NITK/HandwrittenEquationSolver",
      "project_img_url": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTbMjcHRruOCt6rufgfSArkSHIIqbxPz1tcLN4gUVXixlqLYkpEVJqFq2GgARkKRzl0aV0&usqp=CAU",
      "project_description": "As the name of the project suggests, we intend to make such a system(Web Interface) in which user will just upload his/her handwritten equation(s) and in return would get the solution to that.",
      "project_tags": "Python, Deep Learning, MBSA",
      "project_body": "# Handwritten Equation Solver\r\n\r\n\r\n## AIM of the Project\r\nAs the name of the project suggests, we intend to make such a system(Web Interface) in which user will just upload his/her handwritten equation(s) and in return would get the solution to that.\r\n> Note: The project is restricted only for Polynomials & Linear Equations(1 & 2 Variable)\r\n\r\n## Implementation\r\n- For detecting handwritten equation, we need to detect number,mathematical symbols,variables etc. So firstly, we trained a CNN(Convolutional Nueral Network) Model over a specified dataset.\r\n- Now once the CNN Model was tested enough for detecting handwritten contents, we further proceed to apply some Algebra to solve these detected Equations/Polynomials.\r\n- Then, we created a Frontend where user can upload & crop the images of the handwritten equation and can feed it further to the Backend Servers.\r\n- The user uploaded images are way to diffrent from the images that CNN demands,Hence we applied Image processing(OpenCV) on the user uploaded image to convert it into a desired image.\r\n\r\n## Frameworks & Modules Used\r\n- **Tensorflow** & **Keras** (for Training & Testing CNN Model)\r\n- **Flask** Web Framework (for Backend)\r\n- **HTML**,**CSS**,**JS**,**Bootstrap**(for Frontend)\r\n- **OpenCV** Library of Python (for Image Processing)\r\n- **sympy** module(Handles all equation Solving)\r\n\r\n## Working of the Project\r\n> Note: For the project 'a','b' are variables & 'x' is the multiplication sign.Also '=0' is already understood by the model so user need not write it. So if the Equation is 15a=45, then user need to write 15a-45 and then upload it on the website.\r\n- The user uploads an image to the website as shown below:-\r\n![eq](https://user-images.githubusercontent.com/85332648/161968276-040dad35-b81d-4a47-b595-f428496ff800.jpeg)\r\n- Then the image is processed and converted to desired image that model wants as shown below.\r\n![image](https://user-images.githubusercontent.com/85332648/161968517-78d5adaf-96bc-472b-a5c2-7dbab94c7bdf.png)\r\n- Model then detect that an equation with **variable 'a'** has been uploaded & generates the following output.\r\n![image](https://user-images.githubusercontent.com/85332648/161969317-027e503f-a9a7-45b4-b077-d06458b8e9b0.png)\r\n- Similarly for **Linear Equation in two variable** user need to upload two Images as shown below:-\r\n![eq1](https://user-images.githubusercontent.com/85332648/161969873-2dd69652-67b2-4930-9de3-c97184875e3c.jpeg)\r\n![eq2](https://user-images.githubusercontent.com/85332648/161969912-505cc709-95ba-492b-bfd7-dcce54944027.jpeg)\r\n- Again Model coverts both images as desired images & Solves it!!\r\n![image](https://user-images.githubusercontent.com/85332648/161970223-07b7aaa5-ef10-49d9-b28e-d6ff49df8766.png)\r\n![image](https://user-images.githubusercontent.com/85332648/161970272-64c63894-d011-4e15-9264-5f33c4609531.png)\r\n- **Model produces Following output:-**\r\n\r\n![image](https://user-images.githubusercontent.com/85332648/161970375-8d5d9888-e699-4a15-819d-52a759ddd2cc.png)\r\n\r\n## Why CNN?\r\n- In traditional Neural Networks, one need to specify the important features to be considered while **CNN** automatically detects the important features without any **Human Supervision.**\r\n- Our project involved **high level pixel processing** and CNN's are considered to be the best for that as it can learn the key features for any class itself.\r\n- **CNN's** even have better accuracy than any other **Machine Learning Model** when it comes to **\"Detection\"**.Hence CNN was the best thing to be used for this project.\r\n\r\n## Challenges We ran into\r\n- When we trained the CNN Model for **\"=\"** , **\"(\"** & **\")\"** signs, its accuracy reduced significantly which is absurd.\r\n- Introducing variable **\"x\"** was challenging as **\"x\"** also represents multiplication sign. So we introduced **\"a\"** & **\"b\"** as variables.\r\n- User is constrained to write the equation in a paper with white unruled background. Ruled background confuses the CNN Model & it produces unusual results.\r\n\r\n## Steps to Test Backend\r\n- Download the **.ipynb** file from the github & upload it in **Google Colab**.\r\n- Also Do upload the **model** folder in the colab environment.\r\n- Now you may upload your own custom equations in the Colab environment & all other steps are mentioned in the .ipynb file itself.\r\n> Do Ensure that you crop the equation to the maximum extent.\r\n> Do this!!!üëáüëáüëá\r\n\r\n![2](https://user-images.githubusercontent.com/85332648/161974694-5f5fb2a0-1244-47f1-b9c5-3164d41d216c.jpeg)\r\n> Not this!!!üëáüëáüëá\r\n\r\n![eq7](https://user-images.githubusercontent.com/85332648/161974775-dd58c278-7488-45ca-b272-f08e15a5ce46.jpeg)"
    },
    {
      "id": 24,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/24/",
        "html_url": "http://localhost/projects/sudoku-solver/",
        "slug": "sudoku-solver",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-02-28T16:04:35.812062Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Sudoku Solver",
      "project_title": "Sudoku Solver",
      "project_sig": "CIPHER",
      "project_slug": "sudokusolver",
      "project_year": 2023,
      "project_authors": "Monika Agarwal, Verupaka Yashwanth Reddy, Adith Shekhar Gatty, Raunak Somani, Pragya Kiran",
      "github_url": "https://github.com/IET-NITK/SudokuSolver",
      "project_img_url": "https://images.unsplash.com/photo-1640537702474-3e503c21eefc?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D",
      "project_description": "Sudoku is a logic-based puzzle where the user is given a 9 x 9 grid, which consists of nine 3 x 3 sub-grids, and the objective of the puzzle is to fill these sub-grids with numbers from 1 to 9 such that no two numbers appear in the same row, same column and the same sub-grid of the 9 x 9 grid.",
      "project_tags": "Sudoku, Logic, Maths",
      "project_body": "# Sudoku Solver\r\n\r\n## Introduction\r\nSudoku is a logic-based puzzle where the user is given a 9 x 9 grid, which consists of nine 3 x 3 sub-grids, and the objective of the puzzle is to fill these sub-grids with numbers from 1 to 9 such that no two numbers appear in the same row, same column and the same sub-grid of the 9 x 9 grid. Initially, the grid is partially filled with digits in specific locations to make sure the puzzle is well-posed and has a single solution. This is a very popular puzzle that has been featured in newspapers since the 19th century, the first being featured in a French newspaper. The puzzle can range in difficulty from easy to hard and is also quite addictive. There are many people who start off their typical day by reading the newspaper in the morning and solving puzzles like Sudoku. The Sudoku solver application aids the users to obtain a solution of a sudoku puzzle by providing an incomplete sudoku puzzle as the input. The further sections will cover the functionality and the working of the Sudoku solver application.\r\n\r\n## Functionality of the Application\r\nThe Sudoku solver application takes the incomplete Sudoku as the input and, if valid, solves the Sudoku and displays it; otherwise displays the error message indicating invalid input.\r\nThe application enables the user to provide input in two ways:\r\n\r\n- **Camera/Photo upload:** User has the option to click or the upload the photo of the sudoku as the input\r\n- **Manual input:** User needs to manually enter the numbers in the corresponding boxes as the input. If the given input is solvable or valid and the application solves and displays the complete sudoku. Otherwise it will give out a message indicating that the sudoku was invalid i.e there was no solution possible in such a way that it follows all the rules of a classic sudoku puzzle.\r\n\r\n## Working of the Application\r\nThe basic user interface of the application was developed using Flutter, which is a software development kit created by Google and uses the Dart programming language to develop applications. This app consists of 2 tabs and the user between these tabs (by default the first tab remains activated) by clicking the respective button in the bottom navigation bar. The two tabs are:\r\n\r\n1. **Scan/Upload tab:**\r\nThis tab enables the user to provide an input via an image. They need to click the camera icon to take a photo and click the image icon to upload a photo. Once the required photo is clicked or uploaded, they are given an option to crop the image and they must crop out only the sudoku puzzle out of that image(Important). The cropped out sudoku puzzle is then displayed on the screen. This feature of providing an image as the input by taking a picture or uploading an image and then cropping out the image was made possible by using the **document_scanner_flutter 0.2.5 **dart package. The user must click the ‚ÄòOK‚Äô button below the displayed image to start the text scanning process. For this functionality **google_ml_vision 0.0.7 **dart package was used which returned all the scanned text and its corresponding bounding box (coordinates of the box around the text in pixels) from the image. With this, their respective index is calculated and filled in the 9 x 9 matrix. After the scanning process the application directs the user to the second tab.\r\n2. **Fill tab:**\r\nThis tab initially displays an empty sudoku i.e 9 x 9 boxes or cells with no values. The user has the option to enter a value in the cell by clicking the particular cell (which highlights the cell) and then clicking the button with their required number. There are also options to clear a selected cell or reset the whole sudoku. Note: Changes in the sudoku cause the corresponding changes in the 9 x 9 matrix that was initially declared as a zero 9 x 9 matrix (where 0 indicates no value at that particular position). If the user has given the input through 1st Tab, they are directed to the second tab and the corresponding changes in the 9 x 9 matrix is reflected in the displayed sudoku. The user can make some changes to make sure it matches the input sudoku in case of some errors. Note: The result from scanning isn‚Äôt always accurate and might miss some numbers or display wrong numbers in the wrong position. Finally the user must click the submit button which sends the 9 x 9 matrix to a function to implement the sudoku solving algorithm.\r\n\r\n**Sudoku solving algorithm:**\r\nThe main algorithm used in the process is **Backtracking**. we have made some sub functions inside the code to solve the different part of algorithm for sudoku solver including :\r\n\r\n**IsValid():** This function is basically made for checking the validity of a particular number in a specific index of 9 X 9 grid for correct sudoku. It takes input as grid, number, rowindex, columnindex and gives boolean validation.\r\n\r\n**SolveSudoku():** This function is basically made for solving 9 X 9 grid into correct sudoku. It terminates when the index reaches the end of the grid and also handles the boundary conditions.\r\nThe final output will depend on the return statement where true will display the matrix in the form of sudoku and false will display a message indicating invalid input.\r\n\r\n## Dart packages used in the application:\r\n\r\n- [google_ml_vision(v0.0.7)](https://pub.dev/packages/google_ml_vision)\r\n- [document_scanner_flutter(v0.2.5)](https://pub.dev/packages/document_scanner)"
    },
    {
      "id": 25,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/25/",
        "html_url": "http://localhost/projects/youtube-transcript-summarizer/",
        "slug": "youtube-transcript-summarizer",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-02-28T16:07:04.847112Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "YouTube Transcript Summarizer",
      "project_title": "YouTube Transcript Summarizer",
      "project_sig": "CIPHER",
      "project_slug": "youtubetranscriptsummarizer",
      "project_year": 2023,
      "project_authors": "Suyash Satish Chintawar, Feyaz Baker, Venkat Rohit Merugu, Naveen Kumar, Gaurav Kumar, Sahana",
      "github_url": "https://github.com/IET-NITK/YT-Transcript-Summarizer",
      "project_img_url": "https://images.unsplash.com/photo-1521302200778-33500795e128?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D",
      "project_description": "Throughout the day, an enormous number of video recordings are generated and shared on the Internet. It has become quite difficult to devote time to watching such videos, which may last longer than expected, and our efforts may be in vain if we are unable to extract useful information from them.",
      "project_tags": "Youtube",
      "project_body": "# YouTube Transcript Summarizer\r\n\r\n\r\n## Introduction\r\nThroughout the day, an enormous number of video recordings are generated and shared on the Internet. It has become quite difficult to devote time to watching such videos, which may last longer than expected, and our efforts may be in vain if we are unable to extract useful information from them.\r\n\r\nSummarizing transcripts of such videos allows us to rapidly look for relevant patterns in the video and saves us time and effort from having to go through the entire content. This project will allow us to gain hands-on experience with cutting-edge NLP techniques for abstractive text summarization.\r\n\r\n## Dependencies and Requirements\r\n- `flask==2.0.2`\r\n- `torch==1.10.2`\r\n- `urllib3==1.26.8`\r\n- `transformers==4.16.2`\r\n- `huggingface-hub==0.4.0`\r\n- `youtube-transcript-api==0.4.3`\r\n- `bert-extractive-summarizer==0.10.1`\r\n\r\n## Running the website\r\n1. Clone repository\r\n\r\n`git clone https://github.com/IET-NITK/IETC-YT-Transcript-Summarizer`\r\n\r\n2. Install requirements\r\n\r\n`pip install -r requirements.txt`\r\n\r\n3. Run website\r\n\r\n`python3 app.py`\r\n\r\n## Methodology\r\nThree major steps are involved:\r\n\r\n- Fetching Subtitles : ‚Äòyoutube_transcript_api‚Äô has been used to extract summary for a youtube video by passing the video ID as the input to the api.\r\n- Summarizing transcripts : BERT extractive summarisation has been used to summarize the obtained video transcript.\r\n- Display summary on website : The summary is then displayed on the website hosted on flask server.\r\n\r\n## Challenges\r\n- Finding good abstractive text summarization techniques.\r\n- Time required in summarization."
    },
    {
      "id": 29,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/29/",
        "html_url": "http://localhost/projects/image-driven-gaming/",
        "slug": "image-driven-gaming",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-03-21T16:20:36.938615Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Image Driven Gaming",
      "project_title": "Image Driven Gaming",
      "project_sig": "CIPHER",
      "project_slug": "image-driven-gaming",
      "project_year": 2023,
      "project_authors": "Gautam Sivakumar, Abhijeet Adi, Akash Reddy",
      "github_url": "https://github.com/Cborgg/IET-Project---Image-Based-Gaming",
      "project_img_url": "https://tetragram.codered.cloud/media/images/2_o8VStdC.max-800x600.jpg",
      "project_description": "This project utilizes computer vision techniques to control keyboard input based on hand gestures captured through a webcam. It employs OpenCV for real-time hand detection and tracking, using HSV color space segmentation. Euclidean distance and slope calculation help recognize specific hand gestures. A custom control module translates these gestures into keyboard commands, enabling actions such as moving forward (W), backward (S), left (A), and right (D). The system allows intuitive interaction with computers, potentially aiding individuals with disabilities or enhancing user experience in gaming or other applications.",
      "project_tags": "Python, Computer Vision, Image Processing, OpenCV, expo24",
      "project_body": "# Image-Based-Gaming\r\n\r\nWouldn‚Äôt it be fun, if you could use your hand to control the car in a game? So, here we have proposed a computer vision concept to control the game with hand gestures by mapping gestures to the W, A, S, D keyboard keys.\r\n\r\n### Libraries used:\r\n‚Ä¢\tpython 3.x\r\n‚Ä¢\timutils\r\n‚Ä¢\tnumpy\r\n‚Ä¢\topencv2\r\n‚Ä¢\ttime\r\n‚Ä¢\tsklearn\r\n‚Ä¢\tctypes\r\n\r\nThe repository includes three main python(.py) files particularly:\r\n1.\tcontrol.py (Code to map gesture- slope and distance with the keys)\r\n2.\tdirectkeys.py (Code to interact with the keyboard keys. Reference: https://stackoverflow.com/questions/14489013/simulate-python-keypresses-for-controlling-a-game%20#%20http://www.gamespp.com/directx/directInputKeyboardScanCodes.html)\r\n3.\tfinal.py (Main code to take input i.e. hand gesture using background subtraction method, find end points of hand and henceforth distance and slope using convex hull)\r\n\r\n### Steps to run the code:\r\n1.\tDownload the zip file and unzip all in same folder. We have used PyCharm Community Edition to run the codes. You may use other python editors which support the above libraries.\r\n2.\tRun the final.py file.\r\n3.\tAdjust the HSV values using the track bar, so that only your hand is visible.\r\n4.\tAfter that, set the Start track bar to 1. \r\n\r\nYou can now use your hand to control the game. \r\n\r\n### Note: \r\n1.\tPlease use plain white or black background for more accuracy. Also, make sure the room is well lit.\r\n2.\tYou are free to modify the slope and distance values depending upon the gesture you make in control.py.\r\n3.\tYou can map more gestures with keys. Refer to: https://gist.github.com/dretax/fe37b8baf55bc30e9d63"
    },
    {
      "id": 30,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/30/",
        "html_url": "http://localhost/projects/optimisation-of-loads-on-bridge/",
        "slug": "optimisation-of-loads-on-bridge",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-03-21T16:25:31.771475Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Optimisation of Loads On Bridge",
      "project_title": "Optimisation of Loads On Bridge",
      "project_sig": "TORSION",
      "project_slug": "load-optimization",
      "project_year": 2024,
      "project_authors": "Impana, Sakshi Ayush ,Hari Krishana, Vishwananth, Chamundeshwari",
      "github_url": "https://github.com/ekankchhaparwal/Room-Exchange-System",
      "project_img_url": "https://tetragram.codered.cloud/media/images/Optimisation_of_Loads_On_Bridge.max-800x600.png",
      "project_description": "The project involves comprehensive bridge modeling and analysis using Staad.pro software. It begins with defining material properties, applying various loads, and conducting an initial analysis to identify concerns. Subsequently, optimization strategies are researched, implemented, and evaluated through simulations. Sensitivity analysis fine-tunes parameters, leading to a final simulation ensuring safety compliance. A detailed report summarizes findings, emphasizing load optimization benefits and structural performance.",
      "project_tags": "loadoptimization, bridges, structures, expo24",
      "project_body": "# Introduction:\r\n\r\nThe bridge modeling application serves as a comprehensive tool for engineers and\r\ndesigners to create, analyze, and optimize various types of bridges. With a focus on three\r\nspecific bridge models ‚Äì Suspension Cable Bridge, Howe Truss Bridge, and Howe Steel\r\nBridge ‚Äì the application aims to facilitate the structural design and optimization process.\r\nFunctionality of the Application:\r\nThe bridge modeling application offers a range of functionalities tailored to the specific\r\nrequirements of each bridge model:\r\n\r\n-  *Suspension Cable Bridge Model*: \r\nAllows users to create a 3D model of a suspension\r\ncable bridge using Staad.pro. Users can define material properties such as steel cables,\r\nconcrete towers, and structural elements like towers, deck, and cables.\r\n\r\n- *How Truss Bridge Model*: \r\nProvides tools for building a 3D model of a Howe truss bridge,\r\nincluding defining material properties such as timber or steel for truss members, deck, and\r\nother components.\r\n\r\n-  *How Steel Bridge Model*: \r\nEnables users to create a 3D model of a Howe steel bridge,\r\nspecifying material properties such as steel for truss members, deck, and other structural\r\nelements.\r\n\r\n## Manual Input:\r\nUsers have the option to manually input design parameters and specifications for each\r\nbridge model. This includes defining dimensions, material properties, and structural\r\nelements through a user-friendly interface.\r\nWorking of the Application:\r\n\r\nThe application guides users through the following workflow for each bridge model:\r\n\r\n1. *Bridge Modeling*: Users begin by selecting the desired bridge model (Suspension Cable\r\nBridge, Howe Truss Bridge, or Howe Steel Bridge) and proceed to create a 3D model using\r\nStaad.pro. They define material properties such as steel, concrete, timber, etc., and\r\nstructural elements including towers, deck, truss members, etc.\r\n\r\n2. *Load Definition*: After creating the bridge model, users apply various loads such as traffic loads, environmental loads, wind loads, etc., based on the specific requirements of the chosen bridge model. They verify load combinations according to relevant design standards and codes.\r\n\r\n3. *Initial Analysis*: Users conduct a preliminary structural analysis to assess the effects of applied loads on the bridge model. They identify potential areas of concern such as stress concentrations, deflections, stability issues, etc.\r\n\r\n4. *Load Optimization Strategies*: The application provides resources for researching and\r\ndiscussing load optimization techniques relevant to each bridge model. Users select\r\nappropriate strategies for optimizing the bridge design based on project requirements and\r\nconstraints.\r\n\r\n5. *Implementing Optimization*: Users modify the bridge model to incorporate load\r\noptimization changes identified in the previous step. They make necessary adjustments to\r\nstructural elements, material properties, and design configurations.\r\n\r\n6. *Simulation Analysis*: Users evaluate the structural performance of the optimized bridge\r\nmodel under various load conditions. They compare the results with the initial analysis to\r\nassess the effectiveness of load optimization strategies.\r\n\r\n7. *Sensitivity Analysis*: The application facilitates sensitivity analysis to assess the impact of\r\nvarious factors on the bridge's structural performance. Users fine-tune load optimization\r\nparameters if needed based on sensitivity analysis results.\r\nBy following these steps, engineers and designers can efficiently model, analyze, and\r\noptimize Suspension Cable Bridges, Howe Truss Bridges, and Howe Steel Bridges to meet\r\nproject requirements and ensure structural integrity and safety.\r\n\r\n[here](https://drive.google.com/file/d/1l934QzOlXxlwcXcssKchfOsFHRjYHC5L/view?usp=sharing) is the link of  *simulation video*"
    },
    {
      "id": 31,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/31/",
        "html_url": "http://localhost/projects/room-exchange-system/",
        "slug": "room-exchange-system",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-03-21T16:28:57.809911Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Room Exchange System",
      "project_title": "Room Exchange System",
      "project_sig": "CIPHER",
      "project_slug": "roomexchange",
      "project_year": 2024,
      "project_authors": "Ekank Chhaparwal, Srivatsan Suresh, Shiven Dutt Sharma, Anish Tripathi",
      "github_url": "https://github.com/ekankchhaparwal/Room-Exchange-System",
      "project_img_url": "https://tetragram.codered.cloud/media/images/2_o8VStdC.max-800x600.jpg",
      "project_description": "The Room-Exchange-System is a dynamic and user-friendly web application designed to ease the process of finding room exchange opportunities. Our portal addresses these challenges by providing a centralized platform where users can create detailed room listings, browse available listings based on their preferences, and communicate securely with potential room exchange partners. The portal offers a seamless and intuitive solution to facilitate the search.",
      "project_tags": "Web Dev, MySQL, Express, REST API, expo24",
      "project_body": "# Room-Exchange-System\r\n\r\n## Acknowledgements\r\n\r\n- [Srivatsan Suresh](https://github.com/srivatsan-suresh) - Srivatsan implemented the backend functionalities. Created the schema and the backend APIs.\r\n- [Shiven Dutt Sharma](https://github.com/ShivenDuttSharma1) - Shiven contributed to both the frontend and backend functionalities. Created APIs and also made some part of the web design.\r\n- [Anish Tripathi](https://github.com/Anish-Tripathi) - Anish implemented frontend design and user interface components.\r\n-[Ekank Chhaparwal](https://github.com/ekankchhaparwal) - Ekank mentored and guided us throughout the entire project. He integrated both the frontend and the backend.\r\n\r\nLast but not the least, we want to thank the IET community for their encouragement, support, and understanding during the hours spent on the project.\r\n\r\nWe are truly grateful for the contributions and support received from everyone mentioned above, without which the Room-Exchange-System would not have been possible.\r\n\r\n## Aim\r\n\r\nThe aim of the Room-Exchange-System is to provide a platform for individuals seeking to exchange rooms, facilitating the process of finding suitable room exchange partners.\r\n\r\n## Introduction\r\n\r\nThe Room-Exchange-System is a dynamic and user-friendly web application designed to ease the process of finding room exchange opportunities.\r\n\r\nFinding suitable accommodation can be a daunting task, especially when faced with the challenges of compatibility with potential roommates. Traditional methods of searching for roommates or listing available rooms often lack the flexibility and efficiency required to meet the needs of individuals seeking room exchanges.\r\n\r\nOur portal addresses these challenges by providing a centralized platform where users can create detailed room listings, browse available listings based on their preferences, and communicate securely with potential room exchange partners. The portal offers a seamless and intuitive solution to facilitate the search.\r\n\r\n\r\n\r\n## Methodology\r\n\r\nThe Room Exchange-System employs a user-friendly interface to allow users to create listings for their rooms, browse available listings, and communicate with potential room exchange partners. The portal utilizes a combination of-\r\n\r\n- API Development\r\n\r\n- Version control system- Git/GitHub\r\n\r\n- Front-end Development\r\n\r\n- Database Connectivity\r\n \r\n\r\n## Implementation\r\n\r\nThe Room-Exchange-System is implemented using the following technologies:\r\n\r\n- Backend:\r\n  - Node.js\r\n  - Express.js\r\n  - Mysql\r\n\r\n- Frontend:\r\n  - HTML\r\n  - CSS \r\n  - Javascript\r\n\r\nThe backend handles user authentication, listing management, and messaging/ and room selection functionalities, while the frontend provides an intuitive user interface for interacting with the portal.\r\n\r\n## Results\r\n\r\nThrough the Room-Exchange-System, users can successfully find room exchange partners, facilitating seamless transitions between different rooms. \r\n\r\n## Conclusion\r\n\r\nThe Room Exchange-System could prove to be a valuable tool for individuals seeking to exchange rooms. By simplifying the process of finding room exchange opportunities and facilitating communication between users, the portal could greatly benefit the students.\r\n\r\n## References\r\n\r\n1. https://dev.mysql.com/doc/\r\n2. https://devdocs.io/express/\r\n\r\n\r\n## Mentors and Mentees\r\n\r\n### Mentor - \r\n1. Ekank Chhaparwal\r\n### Mentees\r\n1. Srivatsan Suresh\r\n2. Shiven Dutt Sharma\r\n3. Anish Tripathi\r\n\r\n## Features\r\n\r\n\r\n- Login and Sign Up: Registers a student using email and authenticates using password.\r\n- Profile: Provides the basic details of the logged in student.\r\n- Room listing and availability: It shows list of rooms which are availabe for swaps. The student can also filter based on some conditions. On clicking each room, it provides a brief description of the room.\r\n- Room Swap Requests List: Displays all the swap requests made by the student.\r\n- Gudelines: A set of instructions on how our room swap process works.\r\n- Help and Support: Provides contact details of our support team."
    },
    {
      "id": 32,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/32/",
        "html_url": "http://localhost/projects/dsa-tracker/",
        "slug": "dsa-tracker",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-03-21T16:31:51.305233Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "DSA Tracker",
      "project_title": "DSA Tracker",
      "project_sig": "CIPHER",
      "project_slug": "dsatracker",
      "project_year": 2024,
      "project_authors": "Eshan Kharya , Dev Bhojani, Shriya Sheri, Shreyas, Ajay",
      "github_url": "https://github.com/Ajay-33/DSA_Tracker_React",
      "project_img_url": "https://tetragram.codered.cloud/media/images/2_o8VStdC.max-800x600.jpg",
      "project_description": "A MERN Stack based DSA Tracker application that has a database of DSA questions categorised by topics, an express based backend api that handles CRUD operations and a react based front end that allows a user to change question status, add notes for questions  check solutions, check resources etc.",
      "project_tags": "Web Development, MERN, expo24",
      "project_body": "<h1 align=\"center\">Welcome to A2Z DSA Tracker üëã</h1>\r\n\r\n## üöÄ Aim\r\nThe A2Z DSA Tracker project aims to revolutionize the way you prepare for technical interviews! Our platform offers a comprehensive solution for organizing, tracking, and managing data structures and algorithms (DSA) practice questions.\r\n\r\n## üìù Introduction\r\nSay goodbye to scattered notes and disorganized study materials! The A2Z DSA Tracker is a user-friendly web application designed to help you efficiently prepare for technical interviews. With our centralized platform, you can easily track your progress on DSA questions and stay on top of your preparation.\r\n\r\n## Features\r\n- **User Authentication**: Users can create an account and log in to track their DSA practice progress.\r\n- **Browse Questions**: Users can browse through a collection of DSA questions categorized by topics.\r\n- **Mark Progress**: Users can mark the status of each question (Done, Revisit, Pending) to track their progress.\r\n- **Dashboard**: Users have access to a dashboard displaying their overall progress and statistics.\r\n- **Responsive Design**: The application is responsive and optimized for various screen sizes.\r\n\r\n## Install\r\n\r\n```sh\r\nnpm install\r\n```\r\n\r\n## Usage\r\n\r\n```sh\r\nnpm run both\r\n```\r\n\r\n## Run tests\r\n\r\n```sh\r\nnpm run test\r\n```\r\n\r\n## Technologies Used\r\n- **MongoDB**: Database for storing DSA questions and user data.\r\n- **Express.js**: Backend framework for handling API requests.\r\n- **React**: Frontend library for building user interfaces.\r\n- **Node.js**: Runtime environment for running the backend server.\r\n- **Tailwind CSS**: Utility-first CSS framework for styling the application.\r\n\r\n## üìö References\r\n- [Tailwind CSS Documentation](https://tailwindcss.com/docs)\r\n- [React Documentation](https://reactjs.org/docs)\r\n- [MongoDB Documentation](https://docs.mongodb.com)\r\n- [Express.js Documentation](https://expressjs.com/en/4x/api.html)\r\n- [Node.js Documentation](https://nodejs.org/en/docs)\r\n\r\n## üìä Results\r\nThe A2Z DSA Tracker has been successfully implemented and has received positive feedback from our team during testing. We are confident in its functionality and performance.\r\n\r\n## üéâ Conclusion\r\nWe are excited about the potential impact of the A2Z DSA Tracker. Join us as we continue to refine and improve the platform to make it even more valuable for technical interview preparation.\r\n\r\n## Show your support\r\nIf you found this project helpful, please consider giving it a ‚≠êÔ∏è on GitHub!"
    },
    {
      "id": 34,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/34/",
        "html_url": "http://localhost/projects/real-time-hand-detection-and-control-using-computer-vision/",
        "slug": "real-time-hand-detection-and-control-using-computer-vision",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-03-21T16:37:57.114333Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Real-Time Hand Detection and Control using Computer Vision",
      "project_title": "Real-Time Hand Detection and Control using Computer Vision",
      "project_sig": "CIPHER",
      "project_slug": "hand-detection-realtime",
      "project_year": 2024,
      "project_authors": "Gautam, Abhijeet, Akash",
      "github_url": "https://github.com/Cborgg/IET-Project---Image-Based-Gaming",
      "project_img_url": "https://tetragram.codered.cloud/media/images/2_o8VStdC.max-800x600.jpg",
      "project_description": "Real-Time Hand Detection and Control using Computer Vision is a project aimed at developing a system that can detect and track hand movements in real-time using a webcam feed. The project utilizes computer vision techniques implemented with OpenCV, a popular library for computer vision and image processing tasks in Python.",
      "project_tags": "python, computer vision, opencv, expo24",
      "project_body": "# Image-Based-Gaming\r\n\r\n## Aim\r\nThe primary objective of this project is to develop a real-time gesture-controlled keyboard simulator. By leveraging computer vision techniques, we aim to create an intuitive interface that allows users to control basic keyboard functions through hand gestures captured by a webcam.\r\n\r\n## Introduction\r\nGesture control has emerged as an exciting frontier in human-computer interaction, offering a natural and intuitive means of communication with digital systems. In this project, we harness the power of computer vision to detect and interpret hand gestures in real-time. These gestures are then translated into corresponding keyboard inputs, enabling users to interact with applications without physical input devices.\r\n\r\n## Methodology\r\nThe methodology employed in this project encompasses several key steps:\r\n\r\n1. **Real-time Video Capture**:\r\n   - Initialize a video capture object using OpenCV (`cv2.VideoCapture(0)`). This object captures frames from the webcam in real-time.\r\n\r\n2. **GUI Initialization**:\r\n   - Create a named window using OpenCV (`cv2.namedWindow('Result')`) to display the processed output.\r\n   - Create trackbars to allow interactive adjustment of HSV (Hue, Saturation, Value) values used for color segmentation.\r\n\r\n3. **Gesture Detection**:\r\n   - Read each frame from the webcam (`cap.read()`), resize it, and flip it horizontally (`cv2.flip(frame, 1)`).\r\n   - Define a region of interest (ROI) within the frame, typically encompassing the area where hand gestures will be performed.\r\n   - Convert the color space of the ROI from BGR to HSV (`cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)`).\r\n\r\n4. **Color Segmentation**:\r\n   - Apply Gaussian blur (`cv2.GaussianBlur`) to reduce noise in the image.\r\n   - Define lower and upper thresholds to create a binary mask (`cv2.inRange`) isolating the desired color (usually skin tones representing the hand).\r\n\r\n5. **Contour Detection**:\r\n   - Apply morphological operations (`cv2.morphologyEx`), such as opening, to the mask to remove small noise and smooth the edges.\r\n   - Detect contours (`cv2.findContours`) within the binary mask, representing the boundaries of objects.\r\n   - Select the largest contour, typically representing the hand, based on its area (`max(contours, key=lambda x: cv2.contourArea(x))`).\r\n\r\n6. **Convex Hull and Extreme Points**:\r\n   - Compute the convex hull of the hand contour (`cv2.convexHull`) to obtain a polygon that encloses the hand region.\r\n   - Identify extreme points of the convex hull, such as the top, bottom, left, and right points, to calculate the center and other parameters.\r\n\r\n7. **Gesture Analysis**:\r\n   - Compute Euclidean distances between key points, such as fingertips and palm center (`pairwise.euclidean_distances`), to analyze gestures.\r\n   - Calculate slopes between specific points to determine hand orientation or gesture direction.\r\n\r\n8. **Keyboard Simulation**:\r\n   - Based on the analyzed gestures, simulate predefined keyboard commands (e.g., W, A, S, D) using the `Control` class (`control.py`).\r\n   - The `Control` class interprets hand gestures and triggers corresponding keyboard commands using the `directkeys.py` module.\r\n\r\n9. **Interactive Control**:\r\n   - Continuously check for user interactions, such as adjusting HSV values through trackbars or activating gesture control with a start button.\r\n   - Upon detecting gestures and analyzing them, simulate keyboard inputs accordingly, allowing users to control applications or games.\r\n\r\n10. **Display and User Feedback**:\r\n    - Display the processed output, including the hand ROI, detected contours, and simulated keyboard inputs, in real-time using OpenCV.\r\n    - Print relevant information, such as Euclidean distances and slopes, to the console for debugging or user feedback purposes.\r\n\r\n11. **Termination**:\r\n    - Continue the main loop until the user presses the 'Esc' key (`k == 27`), at which point release the video capture object and destroy OpenCV windows.\r\n\r\nBy following this methodology, the code achieves real-time gesture-controlled keyboard simulation, providing users with an intuitive and interactive means of interfacing with digital systems.\r\n\r\n\r\n## Implementation\r\nThe implementation of the gesture-controlled keyboard simulator consists of three primary components:\r\n\r\n1. **final.py**: The main script responsible for capturing webcam input, processing hand gestures, and simulating keyboard inputs based on the detected gestures.\r\n2. **control.py**: A module containing the Control class, which interprets hand gestures and triggers appropriate keyboard commands.\r\n3. **directkeys.py**: A utility module providing functions to simulate keyboard key presses and releases using low-level system calls.\r\n\r\n### Libraries used:\r\n‚Ä¢\tpython 3.x\r\n‚Ä¢\timutils\r\n‚Ä¢\tnumpy\r\n‚Ä¢\topencv2\r\n‚Ä¢\ttime\r\n‚Ä¢\tsklearn\r\n‚Ä¢\tctypes\r\n\r\n\r\n### Steps to run the code:\r\n1.\tDownload the zip file and unzip all in same folder. We have used PyCharm Community Edition to run the codes. You may use other python editors which support the above libraries.\r\n2.\tRun the final.py file.\r\n3.\tAdjust the HSV values using the track bar, so that only your hand is visible.\r\n4.\tAfter that, set the Start track bar to 1. \r\n\r\nYou can now use your hand to control the game. \r\n\r\n### Note: \r\n1.\tPlease use plain white or black background for more accuracy. Also, make sure the room is well lit.\r\n2.\tYou are free to modify the slope and distance values depending upon the gesture you make in control.py.\r\n3.\tYou can map more gestures with keys. Refer to: https://gist.github.com/dretax/fe37b8baf55bc30e9d63\r\n\r\n## Results\r\nThrough rigorous testing and experimentation, we have achieved promising results with our gesture-controlled keyboard simulator. Users can effectively control basic keyboard functions using intuitive hand gestures, providing a seamless and engaging interaction experience.\r\n\r\n![image](https://github.com/Cborgg/IET-Project---Image-Based-Gaming/assets/118260187/a0ed582f-11a0-4d9b-bb29-ea52829746cc)\r\n\r\n\r\n## Conclusion\r\nGesture control represents a compelling approach to human-computer interaction, offering benefits such as natural interaction, accessibility, and hands-free operation. While our project focuses on a basic implementation of gesture-controlled keyboard input, the underlying principles can be extended and adapted for various applications, including gaming, accessibility tools, and interactive interfaces.\r\n\r\n## References\r\nFor further exploration and understanding of the technologies and methodologies employed in this project, we recommend the following resources:\r\n\r\n- [OpenCV Documentation](https://opencv.org/)\r\n- [NumPy Documentation](https://numpy.org/doc/)\r\n- [scikit-learn Documentation](https://scikit-learn.org/stable/documentation.html)\r\n- [Code to interact with the keyboard keys](https://stackoverflow.com/questions/14489013/simulate-python-keypresses-for-controlling-a-game%20#%20http://www.gamespp.com/directx/directInputKeyboardScanCodes.html)\r\n\r\n\r\n## Mentors and Mentees\r\nMentees:\r\nGautam Sivakumar\r\nAbhijeet Adi\r\nAkash Reddy"
    },
    {
      "id": 35,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/35/",
        "html_url": "http://localhost/projects/food-waste-reduction-management/",
        "slug": "food-waste-reduction-management",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-03-21T16:39:11.782451Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Food waste reduction management",
      "project_title": "Food waste reduction management",
      "project_sig": "CIPHER",
      "project_slug": "foodwaste-reduction",
      "project_year": 2023,
      "project_authors": "Rounak Jain, Khushi Muddi, Priya Jha, Vikas Kushwaha, Lipika, Sumit",
      "github_url": "https://github.com/ChuckleChic/food-waste-reduction-project",
      "project_img_url": "https://tetragram.codered.cloud/media/images/2_o8VStdC.max-800x600.jpg",
      "project_description": "The food waste reduction project utilizes a donor and receiver platform built with HTML, CSS, and Django backend. Donors can list surplus food items, specifying quantity and expiry dates, while receivers can browse and claim available items for redistribution. The platform facilitates efficient communication and coordination between donors and receivers, reducing food waste by redirecting excess food to those in need. Django manages user authentication, database interactions, and backend logic, ensuring a seamless and secure experience for all participants in the food redistribution process.",
      "project_tags": "HTML, CSS, Javascript, Django, SQL, expo24",
      "project_body": "# Food Waste Reduction Project\r\n\r\n## Aim\r\nA website for Food Waste Reduction\r\n\r\n## Introduction\r\n Develop a mobile application to address the problem of food waste by connecting food providers with local charities, enabling efficient food donation and rescue.\r\n\r\n## Methodology\r\n Web Development: We have created a website with a main home page, with multiple categories in the top bar for Provider and Reciever details, Login/Sign-in, Contact Us. Initial clicking on the either the Provider details view or Reciever details, the web page is redirected to the Login/Sign-In page. The website accepts and shows the profiles of the Providers and Recievers in their respective tabs.\r\n\r\n## Implementation\r\n Front-End: HTML, CSS ; Back-End: Django (python); Tools: GitHub\r\n\r\n## Results\r\n Till date each of the individual tabs are ready and connectivity will be fixed soon.\r\n\r\n### Mentors:\r\nKhushi Muddi\r\nRounak Jain\r\n\r\n### Mentees:\r\nPriya Jha\r\nVikas\r\nSumit\r\nLipika"
    },
    {
      "id": 36,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/36/",
        "html_url": "http://localhost/projects/water-quality-prediction/",
        "slug": "water-quality-prediction",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-03-21T16:40:46.161019Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Water Quality Prediction",
      "project_title": "Water Quality Prediction",
      "project_sig": "TORSION",
      "project_slug": "water-quality-prediction",
      "project_year": 2024,
      "project_authors": "Sharan Vinod Kumar, Chitralekha Singh, Rohit Singh, Darshan, Manas, Saiprasas, Vinesh",
      "github_url": "https://github.com/creativetimofficial/soft-ui-design-system",
      "project_img_url": "https://tetragram.codered.cloud/media/images/2_o8VStdC.max-800x600.jpg",
      "project_description": "It is a ML based project. We train our model on 9 parameters to predict if water is potable or not. Then we collect different samples of water and measure their parameters experimentally. And incorporate that in our model.",
      "project_tags": "Python, ML, expo24",
      "project_body": "It is a ML based project. We train our model on 9 parameters to predict if water is potable or not. Then we collect different samples of water and measure their parameters experimentally. And incorporate that in our model."
    },
    {
      "id": 37,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/37/",
        "html_url": "http://localhost/projects/startup-consultancy/",
        "slug": "startup-consultancy",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-03-21T16:45:09.007177Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Startup Consultancy",
      "project_title": "Startup Consultancy",
      "project_sig": "VENTURE",
      "project_slug": "startup-consultancy",
      "project_year": 2023,
      "project_authors": "Aditya Shanker , Riya Shukla , Ayush Apurv , Abhyuday Rayala",
      "github_url": "https://github.com/Cborgg/IET-Project---Image-Based-Gaming/blob/main/README.md",
      "project_img_url": "https://tetragram.codered.cloud/media/images/2_o8VStdC.max-800x600.jpg",
      "project_description": "Strategic Insights for Fashion Startup Success",
      "project_tags": "Survey Design, Data Analysis Techniques, Statistical Analysis, Data Visualization Tools, Market Research, expo24",
      "project_body": "Project Description: Strategic Insights for Fashion Startup Success\r\n\r\nPart of Our Venture Casebook Initiative at Ventures: Making Ventures a Top Consulting and Finance Club\r\n\r\nOur project focuses on providing strategic insights and actionable recommendations to drive the success of a fashion startup in the accessories industry. Leveraging consulting frameworks, data analysis tools, and industry expertise, we aim to identify market opportunities, understand customer preferences, and develop growth strategies for the startup.\r\n\r\nKey components of our project include problem framing using frameworks such as the McKinsey 7S Framework and Bain 4Ds Framework, data collection through Google Form surveys targeting a diverse sample space including NITK college students and individuals of different age groups and genders, and data analysis using Tableau for visualizations and insights generation.\r\n\r\nOur methodology incorporates analytical techniques such as Porter's Five Forces Analysis and SWOT Analysis to assess market dynamics, competitive forces, internal strengths, weaknesses, external opportunities, and threats. The insights generated from this analysis guide the development of strategic recommendations using frameworks like the Ansoff Matrix, focusing on market penetration, product development, market expansion, and diversification.\r\n\r\nBy applying these methodologies and frameworks, our project aims to deliver a detailed market study report, visualizations and dashboards using Tableau, interactive presentation slides, and actionable recommendations that empower the fashion startup to achieve growth, differentiate itself in the market, and build a strong brand presence."
    },
    {
      "id": 38,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/38/",
        "html_url": "http://localhost/projects/image-upscaling/",
        "slug": "image-upscaling",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-03-21T16:47:14.318920Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Image Upscaling",
      "project_title": "Image Upscaling",
      "project_sig": "CIPHER",
      "project_slug": "image-upscaling",
      "project_year": 2023,
      "project_authors": "B.Anagha, Manvi, Uzzwal, Rahul",
      "github_url": "https://github.com/rahulnb17/image-resolution",
      "project_img_url": "https://tetragram.codered.cloud/media/images/2_o8VStdC.max-800x600.jpg",
      "project_description": "The main idea behind our project is to upscale images using Machine Learning Algorithms. We chose two specific features, the resolution of the image and the brightness that can be enhanced using these algorithms. Super resolution model uses supervised learning to learn how to increase the resolution of the image and successful applies it on the images provided. The brightness enhancement is applied only when a pre-defined function evaluates the image and concludes that it is needed, this is to prevent transforming images into those that have intense glares coming from light sources. This particular phase is applied using an unsupervised model called Zero-DCE. The two models are integrated and deployed using streamlit platform.",
      "project_tags": "Python, ML, DL, Image Processing, expo24",
      "project_body": "# Image Super Resolution\r\n## Motivation for the Project\r\nThis is a project done under IET NITK where the goal is to enhance the brightness of low light image and then pass it to the super resolution model to enhance the visibility and clarity of the image and removing the noise such that the output image is much more clear and visibily pleasing and aesthetically appealing.\r\n### Description\r\n\r\nOur proposed model begins by implementing a mechanism to determine whether the input image exhibits characteristics of low-light conditions.Causes of low-light conditions can be due to insufficient or\r\nabsent light source or uneven illumination caused by back-light and shadows. Subsequently, we will develop a function capable of discerning whether the given image meets the criteria for low-light classification. Upon identification of a low-light image, we will employ the Zero DCE model to enhance its brightness.\r\n\r\nFollowing the enhancement process through the Zero DCE model, the image will undergo further refinement using the Super Resolution model. This subsequent step aims to produce a substantially clearer and denoised version of the image, leveraging the sophisticated capabilities inherent to the Super Resolution model.\r\n\r\nIn essence, our model is designed to automatically detect and address low-light scenarios in images, enhance their brightness using Zero DCE, and further refine them to achieve superior clarity and noise reduction through the Super Resolution model. This holistic approach ensures that images exhibiting low-light conditions are effectively processed to yield optimal visual outcomes.\r\n#### Working\r\n\r\nThe Zero-Reference Deep Curve Estimation (Zero DCE) model is a state-of-the-art method in the field of image enhancement, particularly in addressing low-light conditions. Unlike traditional methods that rely on reference images or prior knowledge, the Zero DCE model operates without any reference input, hence the term \"zero-reference.\"\r\n\r\nAt its core, the Zero DCE model utilizes deep neural networks to predict a transformation curve that can effectively enhance the brightness and visibility of low-light images. This transformation curve is learned directly from the input low-light image itself, without the need for additional reference images or external information.\r\n\r\nThe key innovation of the Zero DCE model lies in its ability to capture and exploit the inherent characteristics of low-light images to generate accurate and effective enhancement curves. By leveraging deep learning techniques, the model can adaptively adjust the brightness levels of pixels in the input image, effectively amplifying details and enhancing visibility without introducing excessive noise or artifacts.\r\n\r\nThe Super Resolution Generative Adversarial Network (SR-GAN) represents a cutting-edge approach to image enhancement, specifically targeted at increasing the resolution and fidelity of low-resolution images. Leveraging the power of Generative Adversarial Networks (GANs), SR-GAN operates on the principle of adversarial training, where two neural networks, namely the generator and the discriminator, engage in a competitive learning process to produce high-quality, high-resolution images.\r\n\r\nThe generator network within SR-GAN is tasked with learning a mapping function that takes a low-resolution image as input and generates a corresponding high-resolution output. Through an iterative process, the generator learns to upscale the image while preserving important details and features. Concurrently, the discriminator network is trained to distinguish between real high-resolution images and those generated by the generator. This adversarial setup encourages the generator to produce increasingly realistic and visually pleasing results.\r\n\r\nOne of the key strengths of SR-GAN lies in its ability to generate photo-realistic images with enhanced resolution, surpassing the capabilities of traditional interpolation-based methods. By harnessing the power of GANs, SR-GAN is able to produce sharp, detailed, and visually appealing images that closely resemble their high-resolution counterparts.\r\n\r\n\r\n## Links\r\n\r\n[link text](https://li-chongyi.github.io/Proj_Zero-DCE.html)\r\n\r\n[link with title](https://pyimagesearch.com/2022/06/06/super-resolution-generative-adversarial-networks-srgan/)"
    }
  ]
}