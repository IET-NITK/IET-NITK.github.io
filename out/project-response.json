{
  "meta": {
    "total_count": 18
  },
  "items": [
    {
      "id": 12,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/12/",
        "html_url": "http://localhost/projects/audio-fingerprinting/",
        "slug": "audio-fingerprinting",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-01-29T15:27:29.458646Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Audio Fingerprinting",
      "project_title": "Audio Fingerprinting",
      "project_sig": "CIPHER",
      "project_slug": "audiofingerprinting",
      "project_year": 2023,
      "project_authors": "Attada Ramprasad, Pooja Gayathri Kanala, Sharuf Baig, K Snehith Bhagavan",
      "github_url": "https://github.com/IET-NITK/AudioFingerprinting",
      "project_img_url": "https://tetragram.codered.cloud/media/images/1_smebNTc.max-800x600.png",
      "project_description": "Recognizing a song from a large cluster of audio can't be achieved by using brute force to compare an audio sample to every song in the database. In this project we use hashing,a process in which reproducible hash tokens are extracted to save the effort.",
      "project_tags": "Audio, Python, Maths",
      "project_body": "# Audio Fingerprinting\r\n\r\n\r\n## ABSTRACT\r\nRecognizing a song from a large cluster of audio can't be achieved by using brute force to compare an audio sample to every song in the database. In this project we use hashing,a process in which reproducible hash tokens are extracted to save the effort, this method compares the hash values instead of the whole ﬁles so that it will be more efficient, Audio fingerprinting is the process of representing an audio signal in a compact way by extracting relevant features of the audio content.The fingerprints from the unknown sample are matched against a large set of fingerprints stored in the database. In this our model we are using mysql as the database.Companies like shazam, phillips, intrasonics and many more use audio fingerprinting for various implementations.\r\n\r\n## AUDIO RECOGNITION\r\nRead the audio file recorded and store it into a 2-D array of amplitude against time.The recording can be done using the microphone or the input file can be read from the system itself and saved as an audio file.This process is same for both , adding the audio files to the database as well as for recognising the audio files i.e both training and testing phase.\r\n\r\n![audio](https://tetragram.codered.cloud/media/images/1_smebNTc.max-800x600.png)\r\n\r\n## Fast Fourier Transform (FFT)\r\nNow we use Fast Fourier Transform (FFT) to change the waveform to frequency domain from time domain.\r\n\r\n![audio](https://tetragram.codered.cloud/media/images/2_aRt4ylH.max-800x600.png)\r\n\r\n## Spectrogram\r\nNext step is to perform Short-Term Fourier Transform (STFT) of the audio signal by breaking down the signal into small chunks and performing the Fourier Transform on each of them to generate the spectrogram, which is a visual plot of all three variables amplitude against time and frequency.\r\n\r\n![audio](https://tetragram.codered.cloud/media/images/3_sZdzF6w.max-800x600.png)\r\n\r\n## Mapping peaks\r\nThe processing is usually carried out on a 2-D array , which stores the STFT coefficients of the file and the peaks, local maxima points of the file, which are mapped by masking.A set of the peak and its neighbour are passed to a hash function to generate a hash. A hash is an encoded string which is unique for each input. An audio fingerprint is generated which is a set of hash values and the offset value(time component of the peak).This value is stored in the database with a unique song_id. After we perform these steps on the known file we can match the audio file.\r\n\r\n## Recognise the song\r\nWe recognise the song by comparing the hash value from the database.A pair of key-value is appended into an empty dictionary created for each song. Where key is the difference between the database offset and the sample offset and value if the number of repetitions of the matches we get while comparing the hash values. A score is calculated for each song which is the maximum value of ‘value’ in that particular dictionary. The song with maximum score is the best match for the input file and the model returns the song name with the score value.\r\n\r\n## Conclusion\r\nThe model has been successful in recognizing the song by finding the fingerprints. The further improvements for this model would be to use noise filter techniques, get accurate fingerprints and improve the database.\r\n\r\n## RESOURCES\r\n- [medium article on audio fingerprinting](https://medium.com/swlh/understanding-audio-fingerprinting-b39682aa3b5f)\r\n- [ourcodeworld.com/shazamclone](https://ourcodeworld.com/articles/read/973/creating-your-own-shazam-identify-songs-with-python-through-audio-fingerprinting-in-ubuntu-18-04)"
    },
    {
      "id": 13,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/13/",
        "html_url": "http://localhost/projects/audio-to-sign-language-translator/",
        "slug": "audio-to-sign-language-translator",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-01-29T15:28:26.615757Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Audio to Sign Language Translator",
      "project_title": "Audio to Sign Language Translator",
      "project_sig": "CIPHER",
      "project_slug": "audiotosignlanguagetranslator",
      "project_year": 2022,
      "project_authors": "Jagan Babu Korra, Vaishnavi Prakash Kalgutkar, K Sriram, Kowshic V, Ansh Bindlish",
      "github_url": "https://github.com/IET-NITK/A2SL-Translator",
      "project_img_url": "https://user-images.githubusercontent.com/76841315/162028693-7aa2e6af-3a7f-4f07-a088-e30628ec754b.png",
      "project_description": "Sign languages are visual languages that use hand, facial and body movements as a means of communication. There are over 135 different sign languages all around the world including American Sign Language (ASL), Indian Sign Language (ISL) and British Sign Language (BSL).",
      "project_tags": "ASL, Audio, Python",
      "project_body": "## Methodology\r\n\r\n- Audio input on the platform using Python Speech Recognition module.\r\n- Conversion of audio to text using Google Speech API.\r\n- Dependency parser for analysing grammatical structure of the sentence and establishing relationship between words.\r\n- ISL Generator: ISL of input sentence using ISL grammar rules.\r\n- Generation of Sign language with concatenating videos\r\n\r\n## Libraries used\r\n- streamlit\r\n- speech_recognition\r\n- cv2\r\n- numpy\r\n- moviepy\r\n- nltk\r\n\r\n## Timeline\r\n\r\n| **STEP** | **TIME NEEDED** |\r\n| --- | ----------- |\r\n| Learning process | 35 days |\r\n| Working on theory to design | 15 days |\r\n| Designing the model/Coding | 25 days |\r\n| Working on changes and simulations\t | 10 days |\r\n| Analysis | 2 days |\r\n\r\n## Challenges\r\n- Compiling a quality repository of words and alphabets for output video.\r\n- Time required in processing video."
    },
    {
      "id": 23,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/23/",
        "html_url": "http://localhost/projects/handwritten-equation-solver/",
        "slug": "handwritten-equation-solver",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-02-28T16:01:50.636454Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Handwritten Equation Solver",
      "project_title": "Handwritten Equation Solver",
      "project_sig": "CIPHER",
      "project_slug": "equationsolver",
      "project_year": 2022,
      "project_authors": "Rachana P J, Dolly Gupta, M B Sai Aditya, Lalit Nagar",
      "github_url": "https://github.com/IET-NITK/HandwrittenEquationSolver",
      "project_img_url": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTbMjcHRruOCt6rufgfSArkSHIIqbxPz1tcLN4gUVXixlqLYkpEVJqFq2GgARkKRzl0aV0&usqp=CAU",
      "project_description": "As the name of the project suggests, we intend to make such a system(Web Interface) in which user will just upload his/her handwritten equation(s) and in return would get the solution to that.",
      "project_tags": "Python, Deep Learning, MBSA",
      "project_body": "# Handwritten Equation Solver\r\n\r\n\r\n## AIM of the Project\r\nAs the name of the project suggests, we intend to make such a system(Web Interface) in which user will just upload his/her handwritten equation(s) and in return would get the solution to that.\r\n> Note: The project is restricted only for Polynomials & Linear Equations(1 & 2 Variable)\r\n\r\n## Implementation\r\n- For detecting handwritten equation, we need to detect number,mathematical symbols,variables etc. So firstly, we trained a CNN(Convolutional Nueral Network) Model over a specified dataset.\r\n- Now once the CNN Model was tested enough for detecting handwritten contents, we further proceed to apply some Algebra to solve these detected Equations/Polynomials.\r\n- Then, we created a Frontend where user can upload & crop the images of the handwritten equation and can feed it further to the Backend Servers.\r\n- The user uploaded images are way to diffrent from the images that CNN demands,Hence we applied Image processing(OpenCV) on the user uploaded image to convert it into a desired image.\r\n\r\n## Frameworks & Modules Used\r\n- **Tensorflow** & **Keras** (for Training & Testing CNN Model)\r\n- **Flask** Web Framework (for Backend)\r\n- **HTML**,**CSS**,**JS**,**Bootstrap**(for Frontend)\r\n- **OpenCV** Library of Python (for Image Processing)\r\n- **sympy** module(Handles all equation Solving)\r\n\r\n## Working of the Project\r\n> Note: For the project 'a','b' are variables & 'x' is the multiplication sign.Also '=0' is already understood by the model so user need not write it. So if the Equation is 15a=45, then user need to write 15a-45 and then upload it on the website.\r\n- The user uploads an image to the website as shown below:-\r\n![eq](https://user-images.githubusercontent.com/85332648/161968276-040dad35-b81d-4a47-b595-f428496ff800.jpeg)\r\n- Then the image is processed and converted to desired image that model wants as shown below.\r\n![image](https://user-images.githubusercontent.com/85332648/161968517-78d5adaf-96bc-472b-a5c2-7dbab94c7bdf.png)\r\n- Model then detect that an equation with **variable 'a'** has been uploaded & generates the following output.\r\n![image](https://user-images.githubusercontent.com/85332648/161969317-027e503f-a9a7-45b4-b077-d06458b8e9b0.png)\r\n- Similarly for **Linear Equation in two variable** user need to upload two Images as shown below:-\r\n![eq1](https://user-images.githubusercontent.com/85332648/161969873-2dd69652-67b2-4930-9de3-c97184875e3c.jpeg)\r\n![eq2](https://user-images.githubusercontent.com/85332648/161969912-505cc709-95ba-492b-bfd7-dcce54944027.jpeg)\r\n- Again Model coverts both images as desired images & Solves it!!\r\n![image](https://user-images.githubusercontent.com/85332648/161970223-07b7aaa5-ef10-49d9-b28e-d6ff49df8766.png)\r\n![image](https://user-images.githubusercontent.com/85332648/161970272-64c63894-d011-4e15-9264-5f33c4609531.png)\r\n- **Model produces Following output:-**\r\n\r\n![image](https://user-images.githubusercontent.com/85332648/161970375-8d5d9888-e699-4a15-819d-52a759ddd2cc.png)\r\n\r\n## Why CNN?\r\n- In traditional Neural Networks, one need to specify the important features to be considered while **CNN** automatically detects the important features without any **Human Supervision.**\r\n- Our project involved **high level pixel processing** and CNN's are considered to be the best for that as it can learn the key features for any class itself.\r\n- **CNN's** even have better accuracy than any other **Machine Learning Model** when it comes to **\"Detection\"**.Hence CNN was the best thing to be used for this project.\r\n\r\n## Challenges We ran into\r\n- When we trained the CNN Model for **\"=\"** , **\"(\"** & **\")\"** signs, its accuracy reduced significantly which is absurd.\r\n- Introducing variable **\"x\"** was challenging as **\"x\"** also represents multiplication sign. So we introduced **\"a\"** & **\"b\"** as variables.\r\n- User is constrained to write the equation in a paper with white unruled background. Ruled background confuses the CNN Model & it produces unusual results.\r\n\r\n## Steps to Test Backend\r\n- Download the **.ipynb** file from the github & upload it in **Google Colab**.\r\n- Also Do upload the **model** folder in the colab environment.\r\n- Now you may upload your own custom equations in the Colab environment & all other steps are mentioned in the .ipynb file itself.\r\n> Do Ensure that you crop the equation to the maximum extent.\r\n> Do this!!!👇👇👇\r\n\r\n![2](https://user-images.githubusercontent.com/85332648/161974694-5f5fb2a0-1244-47f1-b9c5-3164d41d216c.jpeg)\r\n> Not this!!!👇👇👇\r\n\r\n![eq7](https://user-images.githubusercontent.com/85332648/161974775-dd58c278-7488-45ca-b272-f08e15a5ce46.jpeg)"
    },
    {
      "id": 24,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/24/",
        "html_url": "http://localhost/projects/sudoku-solver/",
        "slug": "sudoku-solver",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-02-28T16:04:35.812062Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Sudoku Solver",
      "project_title": "Sudoku Solver",
      "project_sig": "CIPHER",
      "project_slug": "sudokusolver",
      "project_year": 2023,
      "project_authors": "Monika Agarwal, Verupaka Yashwanth Reddy, Adith Shekhar Gatty, Raunak Somani, Pragya Kiran",
      "github_url": "https://github.com/IET-NITK/SudokuSolver",
      "project_img_url": "https://images.unsplash.com/photo-1640537702474-3e503c21eefc?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D",
      "project_description": "Sudoku is a logic-based puzzle where the user is given a 9 x 9 grid, which consists of nine 3 x 3 sub-grids, and the objective of the puzzle is to fill these sub-grids with numbers from 1 to 9 such that no two numbers appear in the same row, same column and the same sub-grid of the 9 x 9 grid.",
      "project_tags": "Sudoku, Logic, Maths",
      "project_body": "# Sudoku Solver\r\n\r\n## Introduction\r\nSudoku is a logic-based puzzle where the user is given a 9 x 9 grid, which consists of nine 3 x 3 sub-grids, and the objective of the puzzle is to fill these sub-grids with numbers from 1 to 9 such that no two numbers appear in the same row, same column and the same sub-grid of the 9 x 9 grid. Initially, the grid is partially filled with digits in specific locations to make sure the puzzle is well-posed and has a single solution. This is a very popular puzzle that has been featured in newspapers since the 19th century, the first being featured in a French newspaper. The puzzle can range in difficulty from easy to hard and is also quite addictive. There are many people who start off their typical day by reading the newspaper in the morning and solving puzzles like Sudoku. The Sudoku solver application aids the users to obtain a solution of a sudoku puzzle by providing an incomplete sudoku puzzle as the input. The further sections will cover the functionality and the working of the Sudoku solver application.\r\n\r\n## Functionality of the Application\r\nThe Sudoku solver application takes the incomplete Sudoku as the input and, if valid, solves the Sudoku and displays it; otherwise displays the error message indicating invalid input.\r\nThe application enables the user to provide input in two ways:\r\n\r\n- **Camera/Photo upload:** User has the option to click or the upload the photo of the sudoku as the input\r\n- **Manual input:** User needs to manually enter the numbers in the corresponding boxes as the input. If the given input is solvable or valid and the application solves and displays the complete sudoku. Otherwise it will give out a message indicating that the sudoku was invalid i.e there was no solution possible in such a way that it follows all the rules of a classic sudoku puzzle.\r\n\r\n## Working of the Application\r\nThe basic user interface of the application was developed using Flutter, which is a software development kit created by Google and uses the Dart programming language to develop applications. This app consists of 2 tabs and the user between these tabs (by default the first tab remains activated) by clicking the respective button in the bottom navigation bar. The two tabs are:\r\n\r\n1. **Scan/Upload tab:**\r\nThis tab enables the user to provide an input via an image. They need to click the camera icon to take a photo and click the image icon to upload a photo. Once the required photo is clicked or uploaded, they are given an option to crop the image and they must crop out only the sudoku puzzle out of that image(Important). The cropped out sudoku puzzle is then displayed on the screen. This feature of providing an image as the input by taking a picture or uploading an image and then cropping out the image was made possible by using the **document_scanner_flutter 0.2.5 **dart package. The user must click the ‘OK’ button below the displayed image to start the text scanning process. For this functionality **google_ml_vision 0.0.7 **dart package was used which returned all the scanned text and its corresponding bounding box (coordinates of the box around the text in pixels) from the image. With this, their respective index is calculated and filled in the 9 x 9 matrix. After the scanning process the application directs the user to the second tab.\r\n2. **Fill tab:**\r\nThis tab initially displays an empty sudoku i.e 9 x 9 boxes or cells with no values. The user has the option to enter a value in the cell by clicking the particular cell (which highlights the cell) and then clicking the button with their required number. There are also options to clear a selected cell or reset the whole sudoku. Note: Changes in the sudoku cause the corresponding changes in the 9 x 9 matrix that was initially declared as a zero 9 x 9 matrix (where 0 indicates no value at that particular position). If the user has given the input through 1st Tab, they are directed to the second tab and the corresponding changes in the 9 x 9 matrix is reflected in the displayed sudoku. The user can make some changes to make sure it matches the input sudoku in case of some errors. Note: The result from scanning isn’t always accurate and might miss some numbers or display wrong numbers in the wrong position. Finally the user must click the submit button which sends the 9 x 9 matrix to a function to implement the sudoku solving algorithm.\r\n\r\n**Sudoku solving algorithm:**\r\nThe main algorithm used in the process is **Backtracking**. we have made some sub functions inside the code to solve the different part of algorithm for sudoku solver including :\r\n\r\n**IsValid():** This function is basically made for checking the validity of a particular number in a specific index of 9 X 9 grid for correct sudoku. It takes input as grid, number, rowindex, columnindex and gives boolean validation.\r\n\r\n**SolveSudoku():** This function is basically made for solving 9 X 9 grid into correct sudoku. It terminates when the index reaches the end of the grid and also handles the boundary conditions.\r\nThe final output will depend on the return statement where true will display the matrix in the form of sudoku and false will display a message indicating invalid input.\r\n\r\n## Dart packages used in the application:\r\n\r\n- [google_ml_vision(v0.0.7)](https://pub.dev/packages/google_ml_vision)\r\n- [document_scanner_flutter(v0.2.5)](https://pub.dev/packages/document_scanner)"
    },
    {
      "id": 25,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/25/",
        "html_url": "http://localhost/projects/youtube-transcript-summarizer/",
        "slug": "youtube-transcript-summarizer",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-02-28T16:07:04.847112Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "YouTube Transcript Summarizer",
      "project_title": "YouTube Transcript Summarizer",
      "project_sig": "CIPHER",
      "project_slug": "youtubetranscriptsummarizer",
      "project_year": 2023,
      "project_authors": "Suyash Satish Chintawar, Feyaz Baker, Venkat Rohit Merugu, Naveen Kumar, Gaurav Kumar, Sahana",
      "github_url": "https://github.com/IET-NITK/YT-Transcript-Summarizer",
      "project_img_url": "https://images.unsplash.com/photo-1521302200778-33500795e128?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D",
      "project_description": "Throughout the day, an enormous number of video recordings are generated and shared on the Internet. It has become quite difficult to devote time to watching such videos, which may last longer than expected, and our efforts may be in vain if we are unable to extract useful information from them.",
      "project_tags": "Youtube",
      "project_body": "# YouTube Transcript Summarizer\r\n\r\n\r\n## Introduction\r\nThroughout the day, an enormous number of video recordings are generated and shared on the Internet. It has become quite difficult to devote time to watching such videos, which may last longer than expected, and our efforts may be in vain if we are unable to extract useful information from them.\r\n\r\nSummarizing transcripts of such videos allows us to rapidly look for relevant patterns in the video and saves us time and effort from having to go through the entire content. This project will allow us to gain hands-on experience with cutting-edge NLP techniques for abstractive text summarization.\r\n\r\n## Dependencies and Requirements\r\n- `flask==2.0.2`\r\n- `torch==1.10.2`\r\n- `urllib3==1.26.8`\r\n- `transformers==4.16.2`\r\n- `huggingface-hub==0.4.0`\r\n- `youtube-transcript-api==0.4.3`\r\n- `bert-extractive-summarizer==0.10.1`\r\n\r\n## Running the website\r\n1. Clone repository\r\n\r\n`git clone https://github.com/IET-NITK/IETC-YT-Transcript-Summarizer`\r\n\r\n2. Install requirements\r\n\r\n`pip install -r requirements.txt`\r\n\r\n3. Run website\r\n\r\n`python3 app.py`\r\n\r\n## Methodology\r\nThree major steps are involved:\r\n\r\n- Fetching Subtitles : ‘youtube_transcript_api’ has been used to extract summary for a youtube video by passing the video ID as the input to the api.\r\n- Summarizing transcripts : BERT extractive summarisation has been used to summarize the obtained video transcript.\r\n- Display summary on website : The summary is then displayed on the website hosted on flask server.\r\n\r\n## Challenges\r\n- Finding good abstractive text summarization techniques.\r\n- Time required in summarization."
    },
    {
      "id": 29,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/29/",
        "html_url": "http://localhost/projects/image-driven-gaming/",
        "slug": "image-driven-gaming",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-03-21T16:20:36.938615Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Image Driven Gaming",
      "project_title": "Image Driven Gaming",
      "project_sig": "CIPHER",
      "project_slug": "image-driven-gaming",
      "project_year": 2024,
      "project_authors": "Gautam Sivakumar, Abhijeet Adi, Akash Reddy",
      "github_url": "https://github.com/Cborgg/IET-Project---Image-Based-Gaming",
      "project_img_url": "https://tetragram.codered.cloud/media/images/Image_Driven_Gaming.max-800x600.jpg",
      "project_description": "This project utilizes computer vision techniques to control keyboard input based on hand gestures captured through a webcam. It employs OpenCV for real-time hand detection and tracking, using HSV color space segmentation. Euclidean distance and slope calculation help recognize specific hand gestures. A custom control module translates these gestures into keyboard commands, enabling actions such as moving forward (W), backward (S), left (A), and right (D). The system allows intuitive interaction with computers, potentially aiding individuals with disabilities or enhancing user experience in gaming or other applications.",
      "project_tags": "Python, Computer Vision, Image Processing, OpenCV, expo24",
      "project_body": "# Image Based Gaming\r\n\r\nWouldn’t it be fun, if you could use your hand to control the car in a game? So, here we have proposed a computer vision concept to control the game with hand gestures by mapping gestures to the W, A, S, D keyboard keys.\r\n\r\n### Libraries used:\r\n•\tpython 3.x\r\n•\timutils\r\n•\tnumpy\r\n•\topencv2\r\n•\ttime\r\n•\tsklearn\r\n•\tctypes\r\n\r\nThe repository includes three main python(.py) files particularly:\r\n1.\tcontrol.py (Code to map gesture- slope and distance with the keys)\r\n2.\tdirectkeys.py (Code to interact with the keyboard keys. Reference: [Check this out](https://stackoverflow.com/questions/14489013/simulate-python-keypresses-for-controlling-a-game%20#%20http://www.gamespp.com/directx/directInputKeyboardScanCodes.html))\r\n3.\tfinal.py (Main code to take input i.e. hand gesture using background subtraction method, find end points of hand and henceforth distance and slope using convex hull)\r\n\r\n### Steps to run the code:\r\n1.\tDownload the zip file and unzip all in same folder. We have used PyCharm Community Edition to run the codes. You may use other python editors which support the above libraries.\r\n2.\tRun the final.py file.\r\n3.\tAdjust the HSV values using the track bar, so that only your hand is visible.\r\n4.\tAfter that, set the Start track bar to 1. \r\n\r\nYou can now use your hand to control the game. \r\n\r\n### Note: \r\n1.\tPlease use plain white or black background for more accuracy. Also, make sure the room is well lit.\r\n2.\tYou are free to modify the slope and distance values depending upon the gesture you make in control.py.\r\n3.\tYou can map more gestures with keys. Refer to: [website](https://gist.github.com/dretax/fe37b8baf55bc30e9d63)"
    },
    {
      "id": 30,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/30/",
        "html_url": "http://localhost/projects/optimisation-of-loads-on-bridge/",
        "slug": "optimisation-of-loads-on-bridge",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-03-21T16:25:31.771475Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Optimisation of Loads On Bridge",
      "project_title": "Optimisation of Loads On Bridge",
      "project_sig": "TORSION",
      "project_slug": "load-optimization",
      "project_year": 2024,
      "project_authors": "Impana, Sakshi Ayush ,Hari Krishana, Vishwananth, Chamundeshwari",
      "github_url": "https://github.com/ekankchhaparwal/Room-Exchange-System",
      "project_img_url": "https://tetragram.codered.cloud/media/images/Optimisation_of_Loads_On_Bridge.max-800x600.png",
      "project_description": "The project involves comprehensive bridge modeling and analysis using Staad.pro software. It begins with defining material properties, applying various loads, and conducting an initial analysis to identify concerns. Subsequently, optimization strategies are researched, implemented, and evaluated through simulations. Sensitivity analysis fine-tunes parameters, leading to a final simulation ensuring safety compliance. A detailed report summarizes findings, emphasizing load optimization benefits and structural performance.",
      "project_tags": "loadoptimization, bridges, structures, expo24",
      "project_body": "# Introduction:\r\n\r\nThe bridge modeling application serves as a comprehensive tool for engineers and\r\ndesigners to create, analyze, and optimize various types of bridges. With a focus on three\r\nspecific bridge models – Suspension Cable Bridge, Howe Truss Bridge, and Howe Steel\r\nBridge – the application aims to facilitate the structural design and optimization process.\r\nFunctionality of the Application:\r\nThe bridge modeling application offers a range of functionalities tailored to the specific\r\nrequirements of each bridge model:\r\n\r\n-  *Suspension Cable Bridge Model*: \r\nAllows users to create a 3D model of a suspension\r\ncable bridge using Staad.pro. Users can define material properties such as steel cables,\r\nconcrete towers, and structural elements like towers, deck, and cables.\r\n\r\n- *How Truss Bridge Model*: \r\nProvides tools for building a 3D model of a Howe truss bridge,\r\nincluding defining material properties such as timber or steel for truss members, deck, and\r\nother components.\r\n\r\n-  *How Steel Bridge Model*: \r\nEnables users to create a 3D model of a Howe steel bridge,\r\nspecifying material properties such as steel for truss members, deck, and other structural\r\nelements.\r\n\r\n## Manual Input:\r\nUsers have the option to manually input design parameters and specifications for each\r\nbridge model. This includes defining dimensions, material properties, and structural\r\nelements through a user-friendly interface.\r\nWorking of the Application:\r\n\r\nThe application guides users through the following workflow for each bridge model:\r\n\r\n1. *Bridge Modeling*: Users begin by selecting the desired bridge model (Suspension Cable\r\nBridge, Howe Truss Bridge, or Howe Steel Bridge) and proceed to create a 3D model using\r\nStaad.pro. They define material properties such as steel, concrete, timber, etc., and\r\nstructural elements including towers, deck, truss members, etc.\r\n\r\n2. *Load Definition*: After creating the bridge model, users apply various loads such as traffic loads, environmental loads, wind loads, etc., based on the specific requirements of the chosen bridge model. They verify load combinations according to relevant design standards and codes.\r\n\r\n3. *Initial Analysis*: Users conduct a preliminary structural analysis to assess the effects of applied loads on the bridge model. They identify potential areas of concern such as stress concentrations, deflections, stability issues, etc.\r\n\r\n4. *Load Optimization Strategies*: The application provides resources for researching and\r\ndiscussing load optimization techniques relevant to each bridge model. Users select\r\nappropriate strategies for optimizing the bridge design based on project requirements and\r\nconstraints.\r\n\r\n5. *Implementing Optimization*: Users modify the bridge model to incorporate load\r\noptimization changes identified in the previous step. They make necessary adjustments to\r\nstructural elements, material properties, and design configurations.\r\n\r\n6. *Simulation Analysis*: Users evaluate the structural performance of the optimized bridge\r\nmodel under various load conditions. They compare the results with the initial analysis to\r\nassess the effectiveness of load optimization strategies.\r\n\r\n7. *Sensitivity Analysis*: The application facilitates sensitivity analysis to assess the impact of\r\nvarious factors on the bridge's structural performance. Users fine-tune load optimization\r\nparameters if needed based on sensitivity analysis results.\r\nBy following these steps, engineers and designers can efficiently model, analyze, and\r\noptimize Suspension Cable Bridges, Howe Truss Bridges, and Howe Steel Bridges to meet\r\nproject requirements and ensure structural integrity and safety.\r\n\r\n[here](https://drive.google.com/file/d/1l934QzOlXxlwcXcssKchfOsFHRjYHC5L/view?usp=sharing) is the link of  *simulation video*"
    },
    {
      "id": 31,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/31/",
        "html_url": "http://localhost/projects/room-exchange-system/",
        "slug": "room-exchange-system",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-03-21T16:28:57.809911Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Room Exchange System",
      "project_title": "Room Exchange System",
      "project_sig": "CIPHER",
      "project_slug": "roomexchange",
      "project_year": 2024,
      "project_authors": "Ekank Chhaparwal, Srivatsan Suresh, Shiven Dutt Sharma, Anish Tripathi",
      "github_url": "https://github.com/ekankchhaparwal/Room-Exchange-System",
      "project_img_url": "https://tetragram.codered.cloud/media/images/2_o8VStdC.max-800x600.jpg",
      "project_description": "The Room-Exchange-System is a dynamic and user-friendly web application designed to ease the process of finding room exchange opportunities. Our portal addresses these challenges by providing a centralized platform where users can create detailed room listings, browse available listings based on their preferences, and communicate securely with potential room exchange partners. The portal offers a seamless and intuitive solution to facilitate the search.",
      "project_tags": "Web Dev, MySQL, Express, REST API, expo24",
      "project_body": "# Room-Exchange-System\r\n\r\n## Acknowledgements\r\n\r\n- [Srivatsan Suresh](https://github.com/srivatsan-suresh) - Srivatsan implemented the backend functionalities. Created the schema and the backend APIs.\r\n- [Shiven Dutt Sharma](https://github.com/ShivenDuttSharma1) - Shiven contributed to both the frontend and backend functionalities. Created APIs and also made some part of the web design.\r\n- [Anish Tripathi](https://github.com/Anish-Tripathi) - Anish implemented frontend design and user interface components.\r\n-[Ekank Chhaparwal](https://github.com/ekankchhaparwal) - Ekank mentored and guided us throughout the entire project. He integrated both the frontend and the backend.\r\n\r\nLast but not the least, we want to thank the IET community for their encouragement, support, and understanding during the hours spent on the project.\r\n\r\nWe are truly grateful for the contributions and support received from everyone mentioned above, without which the Room-Exchange-System would not have been possible.\r\n\r\n## Aim\r\n\r\nThe aim of the Room-Exchange-System is to provide a platform for individuals seeking to exchange rooms, facilitating the process of finding suitable room exchange partners.\r\n\r\n## Introduction\r\n\r\nThe Room-Exchange-System is a dynamic and user-friendly web application designed to ease the process of finding room exchange opportunities.\r\n\r\nFinding suitable accommodation can be a daunting task, especially when faced with the challenges of compatibility with potential roommates. Traditional methods of searching for roommates or listing available rooms often lack the flexibility and efficiency required to meet the needs of individuals seeking room exchanges.\r\n\r\nOur portal addresses these challenges by providing a centralized platform where users can create detailed room listings, browse available listings based on their preferences, and communicate securely with potential room exchange partners. The portal offers a seamless and intuitive solution to facilitate the search.\r\n\r\n\r\n\r\n## Methodology\r\n\r\nThe Room Exchange-System employs a user-friendly interface to allow users to create listings for their rooms, browse available listings, and communicate with potential room exchange partners. The portal utilizes a combination of-\r\n\r\n- API Development\r\n\r\n- Version control system- Git/GitHub\r\n\r\n- Front-end Development\r\n\r\n- Database Connectivity\r\n \r\n\r\n## Implementation\r\n\r\nThe Room-Exchange-System is implemented using the following technologies:\r\n\r\n- Backend:\r\n  - Node.js\r\n  - Express.js\r\n  - Mysql\r\n\r\n- Frontend:\r\n  - HTML\r\n  - CSS \r\n  - Javascript\r\n\r\nThe backend handles user authentication, listing management, and messaging/ and room selection functionalities, while the frontend provides an intuitive user interface for interacting with the portal.\r\n\r\n## Results\r\n\r\nThrough the Room-Exchange-System, users can successfully find room exchange partners, facilitating seamless transitions between different rooms. \r\n\r\n## Conclusion\r\n\r\nThe Room Exchange-System could prove to be a valuable tool for individuals seeking to exchange rooms. By simplifying the process of finding room exchange opportunities and facilitating communication between users, the portal could greatly benefit the students.\r\n\r\n## References\r\n\r\n1. https://dev.mysql.com/doc/\r\n2. https://devdocs.io/express/\r\n\r\n\r\n## Mentors and Mentees\r\n\r\n### Mentor - \r\n1. Ekank Chhaparwal\r\n### Mentees\r\n1. Srivatsan Suresh\r\n2. Shiven Dutt Sharma\r\n3. Anish Tripathi\r\n\r\n## Features\r\n\r\n\r\n- Login and Sign Up: Registers a student using email and authenticates using password.\r\n- Profile: Provides the basic details of the logged in student.\r\n- Room listing and availability: It shows list of rooms which are availabe for swaps. The student can also filter based on some conditions. On clicking each room, it provides a brief description of the room.\r\n- Room Swap Requests List: Displays all the swap requests made by the student.\r\n- Gudelines: A set of instructions on how our room swap process works.\r\n- Help and Support: Provides contact details of our support team."
    },
    {
      "id": 32,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/32/",
        "html_url": "http://localhost/projects/dsa-tracker/",
        "slug": "dsa-tracker",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-03-21T16:31:51.305233Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "DSA Tracker",
      "project_title": "DSA Tracker",
      "project_sig": "CIPHER",
      "project_slug": "dsatracker",
      "project_year": 2024,
      "project_authors": "Eshan Kharya , Dev Bhojani, Shriya Sheri, Shreyas, Ajay",
      "github_url": "https://github.com/Ajay-33/DSA_Tracker_React",
      "project_img_url": "https://tetragram.codered.cloud/media/images/dsa_tracker.max-800x600.jpg",
      "project_description": "A MERN Stack based DSA Tracker application that has a database of DSA questions categorised by topics, an express based backend api that handles CRUD operations and a react based front end that allows a user to change question status, add notes for questions  check solutions, check resources etc.",
      "project_tags": "Web Development, MERN, expo24",
      "project_body": "# Welcome to A2Z DSA Tracker 👋\r\n\r\n\r\n## 🚀 Aim\r\nThe A2Z DSA Tracker project aims to revolutionize the way you prepare for technical interviews! Our platform offers a comprehensive solution for organizing, tracking, and managing data structures and algorithms (DSA) practice questions.\r\n\r\n## 📝 Introduction\r\nSay goodbye to scattered notes and disorganized study materials! The A2Z DSA Tracker is a user-friendly web application designed to help you efficiently prepare for technical interviews. With our centralized platform, you can easily track your progress on DSA questions and stay on top of your preparation.\r\n\r\n## Features\r\n- **User Authentication**: Users can create an account and log in to track their DSA practice progress.\r\n- **Browse Questions**: Users can browse through a collection of DSA questions categorized by topics.\r\n- **Mark Progress**: Users can mark the status of each question (Done, Revisit, Pending) to track their progress.\r\n- **Dashboard**: Users have access to a dashboard displaying their overall progress and statistics.\r\n- **Responsive Design**: The application is responsive and optimized for various screen sizes.\r\n\r\n## Install\r\n\r\n```sh\r\nnpm install\r\n```\r\n\r\n## Usage\r\n\r\n```sh\r\nnpm run both\r\n```\r\n\r\n## Run tests\r\n\r\n```sh\r\nnpm run test\r\n```\r\n\r\n## Technologies Used\r\n- **MongoDB**: Database for storing DSA questions and user data.\r\n- **Express.js**: Backend framework for handling API requests.\r\n- **React**: Frontend library for building user interfaces.\r\n- **Node.js**: Runtime environment for running the backend server.\r\n- **Tailwind CSS**: Utility-first CSS framework for styling the application.\r\n\r\n## 📚 References\r\n- [Tailwind CSS Documentation](https://tailwindcss.com/docs)\r\n- [React Documentation](https://reactjs.org/docs)\r\n- [MongoDB Documentation](https://docs.mongodb.com)\r\n- [Express.js Documentation](https://expressjs.com/en/4x/api.html)\r\n- [Node.js Documentation](https://nodejs.org/en/docs)\r\n\r\n## 📊 Results\r\nThe A2Z DSA Tracker has been successfully implemented and has received positive feedback from our team during testing. We are confident in its functionality and performance.\r\n\r\n## 🎉 Conclusion\r\nWe are excited about the potential impact of the A2Z DSA Tracker. Join us as we continue to refine and improve the platform to make it even more valuable for technical interview preparation.\r\n\r\n## Show your support\r\nIf you found this project helpful, please consider giving it a ⭐️ on GitHub!"
    },
    {
      "id": 33,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/33/",
        "html_url": "http://localhost/projects/dynamic-hand-gesture-detection-using-emg-signals/",
        "slug": "dynamic-hand-gesture-detection-using-emg-signals",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-03-22T07:59:47.392767Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Dynamic Hand Gesture Detection using EMG Signals",
      "project_title": "Dynamic Hand Gesture Detection using EMG Signals",
      "project_sig": "ROVISP",
      "project_slug": "emg-signals",
      "project_year": 2024,
      "project_authors": "Rahul Basu, Sumukha S Shetty, Srinidhi",
      "github_url": "https://github.com/Basu-Rahul/EMG_signal_gesture_classification",
      "project_img_url": "https://tetragram.codered.cloud/media/images/EMG_Signals.max-800x600.jpg",
      "project_description": "The aim of the project is classification of dynamic hand gestures using EMG signals as input and ML implementation to run the analysis . EMG refers to measuring muscle activation via electric potential. From controlling devices without buttons to creating a more interactive virtual reality experience, hand gesture classification is paving the way for intuitive and natural communication between humans and technology.",
      "project_tags": "Biomedical, ML, expo24",
      "project_body": "# EMG Signal Gesture Classification\r\n\r\n## Introduction :\r\n\r\nHand gesture classification breathes life into hand movements, enabling them to control devices, navigate interfaces, or even translate sign language.\r\nImagine using your hand to flip through virtual channels, play music, or have a conversation without speaking. This technology captures hand movements through cameras or sensors, extracts key features like finger positions and palm orientation, and then uses machine learning to decipher the intended gesture.\r\nThis holds immense potential. From controlling devices without buttons to creating a more interactive virtual reality experience, hand gesture classification is paving the way for intuitive and natural communication between humans and technology. It even holds promise for bridging the gap between the hearing and deaf communities through sign language recognition. \r\nMeasuring muscle activation via electric potential, referred to as electromyography (EMG), with the advent of ever shrinking yet more powerful microcontrollers and integrated circuits, EMG circuits and sensors have found their way into prosthetics, robotics and other control systems. \r\nOur particular interest is to see how to do gesture classification which can be later used as input for design of control systems for rehabilitation and making of prosthetic arm(forearm).\r\n\r\n\r\n## Methodology :\r\n\r\nThe aim of the project is classification of dynamic hand gestures using EMG signals as input and ML implementation to run the analysis . Input is taken using electrodes and muscle sensor and analog signals are collected using Aduino UNO. \r\n\r\nGestures we have focused on pattern of 1-2-1 were transition takes place in middle.\r\nEach gesture sample is of average 30s with first 10s, in position '1', then 10s in position '2' and last 10s in position '1' again.\r\nDataset comprises of 22 samples each of 5 gesture patterns.\r\nData collection is done by the project mentees.\r\n The patterns we are detecting are:\r\n \r\na. Open palm - Closed fist - Open palm\r\n\r\nb. Closed fist - Radial deviation - Closed fist\r\n\r\nc. Extension - Flexion - Extension\r\n\r\nd. Supination - Pronation - Supination\r\n\r\ne. Pinch grip - Open palm - Pinch grip\r\n\r\n\r\n\r\n- Method 1:\r\n\r\n### Frequency-domain analysis\r\n\r\nNext we preprocess the data, based on our conditions we found a frequency domain representation gave much better output than time domain. So, we took 240 point DFT of the signal and then normalised it to be used as model input. So we had 120 features(one-sided dft) and 1 target were the gesture observed was given an ordinal encoding. For our model, we took Random Forest Classifier and final result we took as a confusion matrix, to measure the ability to classify the gestures.\r\n\r\n- Method 2:\r\n\r\n### Time-domain analysis\r\n\r\nwe tried to focus on first 400 samples of each reading and made 20 length 20 boxes for each we are calculating RMS, variance, absolute mean, integrated emg. Next, we are grouping them based on each subject; trial wise. Later for first trial of each of the gestures we are modifying the array size from (5,20,4) to (100,4). \r\nLater we divided the collected data in train and test. And train data further into train and validation.\r\nnext we are using it as dataset for models and training it with svc, random forest classifier and cnn and compared the accuracies we obtain from them. \r\n \r\n\r\n\r\n\r\n## Observations & Results:\r\n\r\nFrom both the methods, we could only achieve around 40% to 45% accuracy. We realise the data points are overlapping thus not giving good results.\r\nBut, we realise this is due to constraints we unknowingly set before the sample collection was done:\r\n\r\na.  Using combination of two or four instead of one electrode, would have been better for getting unique signatures among the different gestures.\r\n\r\nb.  The muscle sensor we used gives rectified and smoothed EMG signal (instead of raw EMG signal), which is easier to work with Arduino UNO, but affects greatly in our \r\n    classification.\r\n\r\nc.  The rectified and smoothed EMG signal works quite well for static gesture detection but for dynamic, makes certain features absent. \r\n\r\n\r\n## Conclusion:\r\n\r\nWe got to learn a whole lot of stuff in domains like biomedical signals, machine learning, CNN and likewise. We do get a much clearer picture on real world signals and how hard it is to formulate it into some fixed constraints. We get a good idea of why sophisticated equipments are essential for better feature/signature detection. But it was a somewhat full-filling experience for our first time venturing into this domain. We expect our project can help others who are starting it new."
    },
    {
      "id": 35,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/35/",
        "html_url": "http://localhost/projects/food-waste-reduction-management/",
        "slug": "food-waste-reduction-management",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-03-21T16:39:11.782451Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Food waste reduction management",
      "project_title": "Food waste reduction management",
      "project_sig": "CIPHER",
      "project_slug": "foodwaste-reduction",
      "project_year": 2024,
      "project_authors": "Rounak Jain, Khushi Muddi, Priya Jha, Vikas Kushwaha, Lipika, Sumit",
      "github_url": "https://github.com/ChuckleChic/food-waste-reduction-project",
      "project_img_url": "https://tetragram.codered.cloud/media/images/73325395.max-800x600.png",
      "project_description": "The food waste reduction project utilizes a donor and receiver platform built with HTML, CSS, and Django backend. Donors can list surplus food items, specifying quantity and expiry dates, while receivers can browse and claim available items for redistribution. The platform facilitates efficient communication and coordination between donors and receivers, reducing food waste by redirecting excess food to those in need. Django manages user authentication, database interactions, and backend logic, ensuring a seamless and secure experience for all participants in the food redistribution process.",
      "project_tags": "HTML, CSS, Javascript, Django, SQL, expo24",
      "project_body": "# Food Waste Reduction Project\r\n\r\n## Aim\r\nA website for Food Waste Reduction\r\n\r\n## Introduction\r\n Develop a mobile application to address the problem of food waste by connecting food providers with local charities, enabling efficient food donation and rescue.\r\n\r\n## Methodology\r\n Web Development: We have created a website with a main home page, with multiple categories in the top bar for Provider and Reciever details, Login/Sign-in, Contact Us. Initial clicking on the either the Provider details view or Reciever details, the web page is redirected to the Login/Sign-In page. The website accepts and shows the profiles of the Providers and Recievers in their respective tabs.\r\n\r\n## Implementation\r\n Front-End: HTML, CSS ; Back-End: Django (python); Tools: GitHub\r\n\r\n## Results\r\n Till date each of the individual tabs are ready and connectivity will be fixed soon.\r\n\r\n### Mentors:\r\nKhushi Muddi\r\nRounak Jain\r\n\r\n### Mentees:\r\nPriya Jha\r\nVikas\r\nSumit\r\nLipika"
    },
    {
      "id": 36,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/36/",
        "html_url": "http://localhost/projects/water-quality-prediction/",
        "slug": "water-quality-prediction",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-03-21T16:40:46.161019Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Prediction of Water Quality using Machine Learning",
      "project_title": "Prediction of Water Quality using Machine Learning",
      "project_sig": "TORSION",
      "project_slug": "water-quality-prediction",
      "project_year": 2024,
      "project_authors": "Saiprasad, Darshan, Manas, Vinesh, Rohit, Chitralekha, Sharan",
      "github_url": "https://github.com/sai-kul/Water-Quality",
      "project_img_url": "https://tetragram.codered.cloud/media/images/Water_Quality_Prediction.max-800x600.png",
      "project_description": "This project focuses on the development of a machine-learning-based predictive model for assessing the potability of water. Leveraging water quality data alongside environmental and meteorological variables such as pH, turbidity, and dissolved solids, the proposed model aims to forecast an important parameter- potability of water. The utilization of models, but not limited to - Random Forest, SVM, and Logistic Regression, is used to provide accurate predictions to aid in decision-making and resource allocation for water management authorities.",
      "project_tags": "ML, Water Quality, Environment, expo24",
      "project_body": "# Water Quality Prediction\r\n**Introduction**:\r\n\r\nWater is one of the most crucial resources for most living creatures, including humans, and vital in sustaining life. We drink water daily to survive, so it is of utmost importance to ensure that the water we consume is potable. Most impurities have a certain limit till they are safe to consume, exceeding these limits will cause a threat to life and negatively impact it. Several methods are used to gauge water quality from rivers, lakes, and oceans.\r\nThere are also other different applications for which we need to gauge the quality of water - in the pharmaceutical industry, we need the utmost purity of water; for agriculture and industry, we need water that is not too salty, free of toxins that shouldn't damage the crops and soil and not damage the environment.\r\nWe will be using various Machine Learning algorithms to predict the water quality based on the parameters provided. The algorithms used in this project are Logistic Regression, K-Nearest Neighbours, Decision Tree, and Support Vector Machine (SVM).\r\n\r\n**About the dataset**:\r\n\r\nSource [Dataset](https://www.kaggle.com/datasets/adityakadiwal/water-potability)\r\n\r\nNumber of data points 3275\r\n\r\nNumber of parameters 10\r\n\r\n**About the parameters**:\r\n\r\n1. pH: The pH of water determines whether water is acidic or alkaline. A pH value of 7 is an ideal case as extreme pH values affect the taste of water and might indicate the presence of harmful substances. \r\n2. Hardness: Hardness is the measure of calcium and magnesium salts in the water sample.\r\n3. Total dissolved salts: High levels of TDS in water affect its taste, quality, and odor. Water samples with high TDS may taste salty and brackish making it unsuitable for consumption.\r\n4. Chloramines: Chloramines and chlorides are used as disinfectants to purify public water systems. Chloramines are formed when chlorides react with ammonia. A value of 4 ppm of chloramines is safe for drinking water.\r\n5. Sulfates: Sulfates are naturally occurring substances that are found in minerals, soil, and rocks. Samples with high sulfate content have a noticeably different odor and taste - bitter and rotten egg smell. High values of sulfates have adverse health effects on humans.\r\n6. Conductivity: Pure water is a bad conductor of water due to the absence of ions. An increase in the ions in water enhances the electrical conductivity of water.\r\n7. Organic Carbon: TOC is a measure of the total amount of carbon in organic compounds in pure water. It comes in water by decaying organic matter and synthetic matter in water.\r\n8. Trihalomethanes: They are chemicals found in water when it is treated with chlorides. The concentrations of trihalomethanes vary with the level of organic matter in water, and the amount of chlorine.\r\n9. Turbidity: The turbidity of water depends on the quantity of solid matter present in the suspended state. It is a measure of the light-emitting properties of water.\r\n10. Potability: It indicates whether the water is safe to drink.\r\n\r\n**Experiments**:\r\n\r\nWe tested 7 water samples which were available around our college. We could only test the water on 4 of the 9 input parameters due to the unavailability of equipment so we tested the samples on pH, conductivity, turbidity, and hardness.\r\nThese are the results we obtained.\r\n![Experiments in the lab](https://github.com/sai-kul/Water-Quality/assets/164490846/e3068b50-bdf2-4733-9c5d-2d1f062ab625)\r\n\r\nThe hardness of water was measured in terms of the volume of EDTA required to change the color of the solution of wine red. The solution consisted of 50 mL of water sample, 3 mL of buffer solution, and 2-3 drops of indicator.\r\nThe conductivity of water was measured in units μS.\r\nThe turbidity of water was measured in units NTU.\r\n\r\n**Modeling**:\r\n\r\nWe used 5 machine learning models - Logistic Regression, Decision Tree, Random Forest, KNN, and SVM.\r\nThe accuracy of these models is depicted in the table and bar graph.\r\n\r\n![results](https://github.com/sai-kul/Water-Quality/assets/164490846/9594c029-7495-42c6-b58a-099f245ba6dd)\r\n\r\n![results graph](https://github.com/sai-kul/Water-Quality/assets/164490846/e4259cd7-544e-4183-bccd-3a8ec2c9f602)\r\n\r\n**Conclusion**:\r\n\r\nWe have tested our models on a very small dataset of about 3300 data points so there is scope to improve our models.\r\nWe can also use deep learning methods and neural networks to see improved performance in our models. Also, we have predicted the potability of water using only 9 parameters but we should also try to incorporate more parameters to get more accurate real-time results."
    },
    {
      "id": 37,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/37/",
        "html_url": "http://localhost/projects/startup-consultancy/",
        "slug": "startup-consultancy",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-03-21T16:45:09.007177Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Startup Consultancy",
      "project_title": "Startup Consultancy",
      "project_sig": "VENTURE",
      "project_slug": "startup-consultancy",
      "project_year": 2024,
      "project_authors": "Aditya Shanker , Riya Shukla , Ayush Apurv , Abhyuday Rayala",
      "github_url": "https://github.com/Cborgg/IET-Project---Image-Based-Gaming/blob/main/README.md",
      "project_img_url": "https://tetragram.codered.cloud/media/images/startup.max-800x600.png",
      "project_description": "Strategic Insights for Fashion Startup Success",
      "project_tags": "Survey Design, Data Analysis Techniques, Statistical Analysis, Data Visualization Tools, Market Research, expo24",
      "project_body": "Project Description: Strategic Insights for Fashion Startup Success\r\n\r\nPart of Our Venture Casebook Initiative at Ventures: Making Ventures a Top Consulting and Finance Club\r\n\r\nOur project focuses on providing strategic insights and actionable recommendations to drive the success of a fashion startup in the accessories industry. Leveraging consulting frameworks, data analysis tools, and industry expertise, we aim to identify market opportunities, understand customer preferences, and develop growth strategies for the startup.\r\n\r\nKey components of our project include problem framing using frameworks such as the McKinsey 7S Framework and Bain 4Ds Framework, data collection through Google Form surveys targeting a diverse sample space including NITK college students and individuals of different age groups and genders, and data analysis using Tableau for visualizations and insights generation.\r\n\r\nOur methodology incorporates analytical techniques such as Porter's Five Forces Analysis and SWOT Analysis to assess market dynamics, competitive forces, internal strengths, weaknesses, external opportunities, and threats. The insights generated from this analysis guide the development of strategic recommendations using frameworks like the Ansoff Matrix, focusing on market penetration, product development, market expansion, and diversification.\r\n\r\nBy applying these methodologies and frameworks, our project aims to deliver a detailed market study report, visualizations and dashboards using Tableau, interactive presentation slides, and actionable recommendations that empower the fashion startup to achieve growth, differentiate itself in the market, and build a strong brand presence."
    },
    {
      "id": 38,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/38/",
        "html_url": "http://localhost/projects/image-upscaling/",
        "slug": "image-upscaling",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-03-21T16:47:14.318920Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Image Upscaling",
      "project_title": "Image Upscaling",
      "project_sig": "CIPHER",
      "project_slug": "image-upscaling",
      "project_year": 2024,
      "project_authors": "B.Anagha, Manvi, Uzzwal, Rahul",
      "github_url": "https://github.com/rahulnb17/image-resolution",
      "project_img_url": "https://tetragram.codered.cloud/media/images/initial-upscale-examples.max-800x600.jpg",
      "project_description": "The main idea behind our project is to upscale images using Machine Learning Algorithms. We chose two specific features, the resolution of the image and the brightness that can be enhanced using these algorithms. Super resolution model uses supervised learning to learn how to increase the resolution of the image and successful applies it on the images provided. The brightness enhancement is applied only when a pre-defined function evaluates the image and concludes that it is needed, this is to prevent transforming images into those that have intense glares coming from light sources. This particular phase is applied using an unsupervised model called Zero-DCE. The two models are integrated and deployed using streamlit platform.",
      "project_tags": "Python, ML, DL, Image Processing, expo24",
      "project_body": "# Image Super Resolution\r\n\r\n## Motivation for the Project\r\nThis is a project done under IET NITK where the goal is to enhance the brightness of low light image and then pass it to the super resolution model to enhance the visibility and clarity of the image and removing the noise such that the output image is much more clear and visibily pleasing and aesthetically appealing.\r\n### Description\r\n\r\nOur proposed model begins by implementing a mechanism to determine whether the input image exhibits characteristics of low-light conditions.Causes of low-light conditions can be due to insufficient or\r\nabsent light source or uneven illumination caused by back-light and shadows. Subsequently, we will develop a function capable of discerning whether the given image meets the criteria for low-light classification. Upon identification of a low-light image, we will employ the Zero DCE model to enhance its brightness.\r\n\r\nFollowing the enhancement process through the Zero DCE model, the image will undergo further refinement using the Super Resolution model. This subsequent step aims to produce a substantially clearer and denoised version of the image, leveraging the sophisticated capabilities inherent to the Super Resolution model.\r\n\r\nIn essence, our model is designed to automatically detect and address low-light scenarios in images, enhance their brightness using Zero DCE, and further refine them to achieve superior clarity and noise reduction through the Super Resolution model. This holistic approach ensures that images exhibiting low-light conditions are effectively processed to yield optimal visual outcomes.\r\n#### Working\r\n\r\nThe Zero-Reference Deep Curve Estimation (Zero DCE) model is a state-of-the-art method in the field of image enhancement, particularly in addressing low-light conditions. Unlike traditional methods that rely on reference images or prior knowledge, the Zero DCE model operates without any reference input, hence the term \"zero-reference.\"\r\n\r\nAt its core, the Zero DCE model utilizes deep neural networks to predict a transformation curve that can effectively enhance the brightness and visibility of low-light images. This transformation curve is learned directly from the input low-light image itself, without the need for additional reference images or external information.\r\n\r\nThe key innovation of the Zero DCE model lies in its ability to capture and exploit the inherent characteristics of low-light images to generate accurate and effective enhancement curves. By leveraging deep learning techniques, the model can adaptively adjust the brightness levels of pixels in the input image, effectively amplifying details and enhancing visibility without introducing excessive noise or artifacts.\r\n\r\nThe Super Resolution Generative Adversarial Network (SR-GAN) represents a cutting-edge approach to image enhancement, specifically targeted at increasing the resolution and fidelity of low-resolution images. Leveraging the power of Generative Adversarial Networks (GANs), SR-GAN operates on the principle of adversarial training, where two neural networks, namely the generator and the discriminator, engage in a competitive learning process to produce high-quality, high-resolution images.\r\n\r\nThe generator network within SR-GAN is tasked with learning a mapping function that takes a low-resolution image as input and generates a corresponding high-resolution output. Through an iterative process, the generator learns to upscale the image while preserving important details and features. Concurrently, the discriminator network is trained to distinguish between real high-resolution images and those generated by the generator. This adversarial setup encourages the generator to produce increasingly realistic and visually pleasing results.\r\n\r\nOne of the key strengths of SR-GAN lies in its ability to generate photo-realistic images with enhanced resolution, surpassing the capabilities of traditional interpolation-based methods. By harnessing the power of GANs, SR-GAN is able to produce sharp, detailed, and visually appealing images that closely resemble their high-resolution counterparts.\r\n\r\n\r\n## Links\r\n\r\n[link text](https://li-chongyi.github.io/Proj_Zero-DCE.html)\r\n\r\n[link with title](https://pyimagesearch.com/2022/06/06/super-resolution-generative-adversarial-networks-srgan/)"
    },
    {
      "id": 40,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/40/",
        "html_url": "http://localhost/projects/riscverse64/",
        "slug": "riscverse64",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-03-22T08:17:12.566041Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "RISCVerse64",
      "project_title": "RISCVerse64",
      "project_sig": "ROVISP",
      "project_slug": "riscverse64",
      "project_year": 2024,
      "project_authors": "Suchet, Chinmayee, Chandana, Sanganabasu",
      "github_url": "https://github.com/chandana38/RISCVerse.git",
      "project_img_url": "https://tetragram.codered.cloud/media/images/WhatsApp_Image_2024-03-22_at_13.33.03.max-800x600.jpg",
      "project_description": "The project focuses on implementing the 64-bit RISC-V architecture, specifically the RV64I variant, which is part of the RISC-V Instruction Set Architecture (ISA). RV64I incorporates the 'I' standard extension, encompassing essential integer computational, Load/Store, and control-flow instructions. By adhering to this standardized framework, our endeavor aims to realize a robust and efficient computing environment tailored to the principles of simplicity, modularity, and scalability inherent in the RISC-V philosophy.",
      "project_tags": "RISC V, expo24",
      "project_body": "## Aim:\r\nThe aim of this project is to design and implement a pipelined RISC-V 32-bit core with the Integer Base Instruction Set (RV32I).\r\n\r\n## Introduction:\r\nThe RISC-V architecture is an open-standard instruction set architecture (ISA) adhering to established reduced instruction set computer (RISC) principles. This project specifically targets the 32-bit variant, RV32I. The \"I\" standard extension incorporates the integer instruction set, encompassing integer computational instructions, load/store instructions, and control flow instructions. The processor pipeline employs a five-stage design, consisting of fetch, decode, execute, memory access, and writeback stages.\r\n\r\n## Methodology & Implementation:\r\nThe initial phase of the project involved designing hardware capable of executing the integer instructions outlined in the RV32I Base Integer Instruction Set. The design of a dedicated datapath for each instruction within the RV32I set was undertaken. This involved identifying the functional units required for each instruction and connecting them together. The individual datapaths were then combined to form the complete datapath. Additionally, a 5-stage pipelined architecture was adopted to enhance efficiency and throughput.\r\n\r\nDatapath Block Diagram of a simple 5-stage pipelined RISC processor\r\n![image](https://github.com/chandana38/RISCVerse/assets/156425665/c20784db-3261-4af6-b87e-e4ba2a0c04bc)\r\n\r\n\r\nMentors:\r\nRehan Nasar,\r\nSanganabasu Herur (211EC245),\r\nL Chandana (211EE235)\r\n\r\nMentees:\r\nChinmayee Nagaraj (221EC212),\r\nSuchet Nayak (221EE254)\r\n\r\n\r\n© 2024 RISC-V Core Implementation Team. All rights reserved."
    },
    {
      "id": 41,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/41/",
        "html_url": "http://localhost/projects/crash-and-object-detection-using-esp32/",
        "slug": "crash-and-object-detection-using-esp32",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-03-22T11:00:25.986592Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "Crash and Object Detection using ESP32",
      "project_title": "Crash and Object Detection using ESP32",
      "project_sig": "ROVISP",
      "project_slug": "carsh-object-detection",
      "project_year": 2024,
      "project_authors": "Hithin Chandra,Lochana M S, A. Shishir , Priyanshu Soni",
      "github_url": "https://github.com/pri1712/Crash-and-object-detection",
      "project_img_url": "https://tetragram.codered.cloud/media/images/crash.max-800x600.png",
      "project_description": "The project aims to address the critical issue of delayed or inadequate response to vehicular accidents by creating a crash detection module. This module is designed to swiftly detect sudden impacts using an accelerometer, accurately pinpoint the location of the incident through a NEO-6M GPS module, and trigger timely emergency responses, thus potentially saving lives and reducing the severity of injuries resulting from road accidents.",
      "project_tags": "Hardware, expo24",
      "project_body": "# Vehicle Crash and object detection #\r\n\r\n\r\n![](https://i.imgur.com/ULQOmeQ.jpeg)\r\n\r\n\r\nThis project has a very powerful usecase in day to day life, we've tackled the challenge of creating a crash and object detection system using available hardware technology at a low cost. Our system, designed for various applications including automotive and industrial sectors, combines the power of ESP32 microcontroller, NEO-6M GPS module, IR sensors, and MPU-6050 accelerometer and gyroscope.\r\n\r\nThe primary goal of our system is straightforward: to swiftly identify potential hazards and improve safety standards across multiple environments. We achieved this by thinking of multiple possiblities that could occur and what could be done to avoid them. This includes object detection which will send an alert to your car dashboard if another vehicle is too close. This can be especially useful in rainy or foggy weather where it may be hard to see. \r\n\r\nThe system demonstrated impressive accuracy in hazard detection, with minimal false positives and rapid response times that meet industry safety standards. The NEO-6M GPS module provided precise location data, further enhancing the system's effectiveness in identifying hazards , the data could be sent directly to the concerned authorities to provide and prompt response which could lead to saving of lives.\r\n\r\n\r\nThe accelerometer and the gyroscope together are a powerful combination which is the main powerhouse behind the crash detection system, because when a vehicle crashes either it accelerates or decelerates. In rare cases there also maybe a change in orientation which can be detected by the gyroscope in the MPU 6050 module. Thus these 2 components provide a powerful tool to detect crashes , which then can be reported with the location and severity(using the data from the gyroscope and accelerometer).\r\n\r\nIn summary, our project showcases a promising solution with significant potential to mitigate risks and enhance safety standards in various applications."
    },
    {
      "id": 42,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/42/",
        "html_url": "http://localhost/projects/codecraft/",
        "slug": "codecraft",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-03-22T11:03:05.290151Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "CodeCraft",
      "project_title": "CodeCraft",
      "project_sig": "CIPHER",
      "project_slug": "codecraft",
      "project_year": 2024,
      "project_authors": "Athrav Vats, Adya N A, Sudeep Y M",
      "github_url": "https://github.com/ayush4345/CodeCraft",
      "project_img_url": "https://tetragram.codered.cloud/media/images/codecraft.max-800x600.jpg",
      "project_description": "Revolutionizing coding practice with AI-powered assistance, personalized challenges, and curated learning resources for all skill levels.",
      "project_tags": "Django, React, LLM, AI, JavaScript, Python, expo24",
      "project_body": "# CodeCraft\r\nIET Project Expo's Project\r\n\r\n## Client\r\n\r\n### 1. Landing Page\r\n\r\nIt displays a list of coding problems and provides filtering options based on difficulty level and problem category. The page includes the following features:\r\n\r\n- Problem List: Renders a table displaying the list of coding problems fetched from a database.\r\n- Filtering: Users can filter the problem list based on difficulty levels (Easy, Medium, Hard) and problem categories (e.g., Dynamic Programming, Two Pointers, Array, Linked List, Stack, Binary Search, Binary Tree, Intervals, Sliding Window, Back Tracking).\r\n- Leaderboard: Includes a leaderboard component to display the rankings of users.\r\n\r\n### 2. Chat Page\r\n\r\nThe Chat Page is a real-time chat application that allows users to send and receive messages. It utilizes the Supabase library for authentication, channel subscription, and database operations. The page features the following functionalities:\r\n\r\n- User authentication: Users must be authenticated to access the chat page.\r\n- Real-time messaging: Messages are synced in real-time using Supabase's channel subscription.\r\n- Message history: Initial messages are fetched from the database upon component mount.\r\n- Sending messages: Users can type and send messages, which are inserted into the database.\r\n- Message display: Received messages are displayed with the sender's name, timestamp, and message content.\r\n\r\n### 3. Problem Page\r\n\r\nThe Problem Page is a dynamic page that displays a specific problem. it has multiple component  as follow:\r\n\r\n1. **Footer**:\r\n    - It includes buttons for submitting the user's code and asking for AI assistance.\r\n\r\n2. **Playground**:\r\n    - This serves as the main coding environment.\r\n    - It includes a code editor powered by the `react-codemirror` library, a section for displaying test cases and results, and an AI assistant feature.\r\n    - It fetches problem data from a Supabase database and handles code submission and AI assistance requests.\r\n\r\n3. **Header**:\r\n    - It allows users to select a programming language, toggle full-screen mode, and access settings.\r\n\r\n4. **ProblemDescription**:\r\n    - This displays the problem statement, examples, constraints, and other relevant information for a given programming problem.\r\n    - It also includes functionality for liking, disliking, and starring problems.\r\n\r\n### 4. Profile Page\r\n\r\nThe Profile Page displays the user's profile information, including their rank, points, activity statistics, and problem-solving analytics. It fetches data from the database and renders the following components:\r\n\r\n- User information: Displays the user's name, rank, member since date, and premium status.\r\n- Activity section: Shows the number of solved, liked, disliked, and starred problems.\r\n- Analytics section: Includes a donut chart that visualizes the user's problem-solving progress based on difficulty levels.\r\n\r\n### 5. Authentication Page\r\n\r\nThe Auth Page is responsible for handling user authentication. It includes the following components:\r\n\r\n### Login\r\n\r\n- This component handles the user login functionality.\r\n- It provides input fields for the user to enter their email and password.\r\n- If the login is successful, it displays a success message redirects the user to the root route.\r\n\r\n- It also includes a button to navigate to the \"Forgot Password\" flow and a link to navigate to the \"Sign Up\" flow.\r\n\r\n### Signup\r\n\r\n- This component handles the user registration functionality.\r\n- It provides input fields for the user to enter their personal information (name, profession, institute, age group, email, display name, password, reason for learning coding, and experience level).\r\n- It also includes a link to navigate to the \"Log In\" flow for users who already have an account.\r\n\r\nOverall, these components work together to provide a seamless authentication experience for users.\r\n\r\n### 6. Learning Page\r\n\r\nThis page provides a user-friendly interface to  explore various topics related to data structures and algorithms. It provides a search bar, displays the results, and shows relevant videos and links based on the user's search query.\r\n\r\n\r\n1. **Topic Search**: The component allows users to enter a topic in the search bar. When the user submits the topic, it sends a request to the server to fetch the relevant information.\r\n\r\n2. **Result Display**: Upon receiving the response from the server, the component displays the result as formatted Markdown content using the `react-markdown` library.\r\n\r\n3. **Video Listing**: The component also fetches relevant videos from the YouTube API based on the search query. These videos are displayed in a vertical timeline using the `react-vertical-timeline-component` library.\r\n\r\n4. **Link Listing**: In addition to videos, the component displays a list of useful content and links related to the search query. These links are also presented in a vertical timeline format. For web scraping of links we are using puppeteer. \r\n\r\n5. **Topic History**: The component maintains a history of the user's searched topics. These topics are displayed as buttons on the left sidebar, allowing the user to quickly revisit a specific topic.\r\n\r\n6. **Topic Management**: Users can delete topics from their history by clicking the delete icon next to each topic button.\r\n\r\n\r\n### API Integrations\r\n\r\nThe learning page interacts with multiple APIs:\r\n\r\n1. **Local Server API**: The component sends a POST request to `http://localhost:8081/api` with the search topic as the request body. The server responds with an array of useful links related to the topic.\r\n\r\n2. **Django Rest API**: The component sends a POST request to `http://127.0.0.1:8000/learnaskai` with the search topic, user profession, age, experience, and level as the request body. The Flask server likely processes the request and responds with the result content.\r\n\r\n3. **YouTube API**: The component fetches relevant videos from the YouTube API by sending a GET request to `[Youtube API](https://www.googleapis.com/youtube/v3/search)`. The API responds with an array of video data, which is then displayed in the vertical timeline.\r\n\r\n4. **Supabase**: The component interacts with the Supabase database to store and retrieve user-related data, such as topics, profession, age, experience, and level.\r\n\r\n\r\n### Web Scraping API with Puppeteer and Express.js\r\n\r\nMade a Express.js server that provides an API for web scraping using the Puppeteer library. The server listens on port 8081 and accepts POST requests at the `/api` endpoint with a topic in the request body. It then uses the Puppeteer library to launch a headless Chrome browser, search for the given topic on Google, and retrieve a list of links from the search results.\r\n\r\n\r\n- `express`: A popular web application framework for Node.js.\r\n- `cors`: A middleware that provides a Connect/Express middleware for handling Cross-Origin Resource Sharing (CORS).\r\n- `puppeteer-core`: A high-level API to control headless Chrome or Chromium over the DevTools Protocol.\r\n\r\n### 7. Practice Contests\r\n\r\n#### Features:\r\n\r\n1. **Challenge List**: A user can choose any number of questions from the list of available questions on the coding platform.\r\n\r\n2. **Countdown Timer**: A countdown timer is displayed at the top of the page, showing the remaining time for the contest. The timer is calculated based on the start time and duration of the contest.\r\n\r\n3. **Contest Points**: After the contest ends, the total points earned by the user during the contest are also displayed.\r\n\r\n### Client Setup and Installation\r\n\r\nTo run this project locally, follow these steps:\r\n\r\n1. Clone the repository: \r\n``` \r\ngit clone https://github.com/ayush4345/CodeCraft.git\r\n ```\r\n\r\n2. Navigate to the project directory: \r\n```\r\ncd client\r\n```\r\n\r\n3. Install dependencies: \r\n```\r\nnpm install\r\n```\r\n4. Set up the Supabase project and configure the required credentials in the `supabase.js` file.\r\n5. Start the development server: \r\n```\r\nnpm start\r\n```\r\n6. For running Puppteer backend, Navigate to the project directory: \r\n```\r\ncd puppeteer\r\n```\r\n\r\n7. Start the puppeteer server : \r\n```\r\nnode server.js\r\n```\r\n\r\nThe project should now be running on `http://localhost:3000`.\r\n\r\n### Dependencies\r\n\r\nThe project relies on the following major dependencies:\r\n\r\n- React\r\n- React Router\r\n- Supabase\r\n- Tailwind CSS\r\n- React Icons\r\n- React Donut Chart\r\n\r\nPlease refer to the `package.json` file for the complete list of dependencies and their versions.\r\n\r\n\r\n## Server \r\n\r\n### Setup Instructions:\r\n\r\n1.Create a virtual environment (optional but recommended):\r\n \r\n```bash\r\npython3 -m venv venv\r\n```\r\n \r\n2.Activate the virtual environment:\r\n\r\n- For Windows:\r\n```bash\r\n.\\venv\\Scripts\\activate\r\n```\r\n- For macOS/Linux:\r\n\r\n```bash\r\nsource venv/bin/activate\r\n```\r\n3.Install requirements from requirements.txt:\r\n\r\n```bash\r\npip install -r requirements.txt\r\n```\r\n\r\n4.Change into the project directory:\r\n\r\n```bash\r\ncd Server\r\n```\r\n\r\n5.Make migrations:\r\n```bash\r\npython3 manage.py makemigrations\r\n```\r\n\r\n6.Apply database migrations:\r\n```bash\r\npython3 manage.py migrate\r\n```\r\n\r\nRun the server:\r\n\r\n```bash\r\npython3 manage.py runserver\r\n```\r\n\r\n## Django REST APIs\r\n\r\nThis repository contains a set of Django REST APIs for various functionalities. The APIs are built using the Django REST Framework and can be accessed through HTTP requests.\r\n\r\n\r\n\r\n### 1. Compile API\r\n\r\n- **URL:** `/compile`\r\n- **Method:** `POST`\r\n- **Request Body:**\r\n  ```json\r\n  {\r\n    \"source_code\": \"print('hello')\",\r\n    \"language_id\": 71,\r\n    \"inputs\": \"\",\r\n    \"outputs\": \"hello\\n\"\r\n  }\r\n  ```\r\n- **Response:**\r\n  ```json\r\n  {\r\n    \"output\": \"hello\\n\",\r\n    \"time\": \"0.013\",\r\n    \"status\": {\r\n      \"id\": 3,\r\n      \"description\": \"Accepted\"\r\n    }\r\n  }\r\n  ```\r\n- **Description:** This API compiles and executes the provided source code in the specified programming language . It returns the output of the code execution, along with the execution time and a status indicator.\r\n\r\n### 2. Languages API\r\n\r\n- **URL:** `/languages`\r\n- **Method:** `GET`\r\n- **Response:**\r\n  ```json\r\n  [\r\n    {\r\n      \"id\": 45,\r\n      \"name\": \"Assembly (NASM 2.14.02)\"\r\n    },\r\n    {\r\n      \"id\": 46,\r\n      \"name\": \"Bash (5.0.0)\"\r\n    },\r\n    // ... (Additional language entries)\r\n  ]\r\n  ```\r\n- **Description:** This API retrieves a list of programming languages supported by the system, along with their respective IDs and names.\r\n\r\n### 3. Ask AI API\r\n\r\n- **URL:** `/askai`\r\n- **Method:** `POST`\r\n- **Request Body:**\r\n  ```json\r\n  {\r\n    \"error\": \"Exception in thread 'main' java.lang.ArrayIndexOutOfBoundsException: Index 9 out of bounds for length 9\\nat Rough.djkstra (Rough.java:12)\\nat Rough.main(Rough.java:75)\",\r\n    \"profession\": \"student\",\r\n    \"age\": \"21\",\r\n    \"experience\": 2,\r\n    \"level\": \"intermidiate\",\r\n    \"prev_response\": \"\"\r\n  }\r\n  ```\r\n- **Response:**\r\n  ```json\r\n  {\r\n    \"content\": \"1. Check the logic for accessing array elements and ensure it is within the bounds of the array.\\n2. Consider the conditions under which the array index is being calculated or manipulated.\\n3. Review how the array is being initialized and populated to identify any potential issues.\"\r\n  }\r\n  ```\r\n- **Description:** This API accepts an error message, along with some user information, and provides a response with suggestions or explanations related to the error.\r\n\r\n### 4. Learn Ask AI API\r\n\r\n- **URL:** `/learnaskai`\r\n- **Method:** `POST`\r\n- **Request Body:**\r\n  ```json\r\n  {\r\n    \"topic\": \"merge sort\",\r\n    \"profession\": \"student\",\r\n    \"age\": \"21\",\r\n    \"experience\": 2,\r\n    \"level\": \"intermidiate\"\r\n  }\r\n  ```\r\n- **Response:**\r\n  ```json\r\n  {\r\n    \"content\": \"### Merge Sort\\n\\nMerge Sort is a classic sorting algorithm known for its efficiency and stability. It follows the divide-and-conquer strategy to sort an array. The algorithm works by recursively dividing the array into two halves until each sub-array contains only one element. Then, it merges the sub-arrays in a sorted manner to produce the final sorted array. Merge Sort has a time complexity of O(n log n), making it suitable for sorting large datasets efficiently.\\n\\n```python\\ndef merge_sort(arr):\\n    if len(arr) > 1:\\n        mid = len(arr) // 2\\n        left_half = arr[:mid]\\n        right_half = arr[mid:]\\n\\n        merge_sort(left_half)\\n        merge_sort(right_half)\\n\\n        i = j = k = 0\\n\\n        while i < len(left_half) and j < len(right_half):\\n            if left_half[i] < right_half[j]:\\n                arr[k] = left_half[i]\\n                i += 1\\n            else:\\n                arr[k] = right_half[j]\\n                j += 1\\n            k += 1\\n\\n        while i < len(left_half):\\n            arr[k] = left_half[i]\\n            i += 1\\n```\"\r\n  }\r\n  ```\r\n- **Description:** This API provides an explanation and example implementation of the Merge Sort algorithm in Python, based on the requested topic and user information.\r\n\r\n\r\n### Working of Django Framework\r\n\r\n1. **Views**:\r\n    Views in Django serve as the endpoints for handling incoming HTTP requests and generating appropriate responses. Views were responsible for implementing the logic behind each API endpoint. For example, we have views for compiling code, retrieving supported languages, and providing AI-based suggestions for error messages.\r\n\r\n2. **URL Routing**:\r\n    URL routing configuration was utilized to map the API endpoints to their respective views. This routing mechanism ensures that incoming requests are directed to the appropriate view for processing. For instance, endpoints like `/compile` and `/languages` were routed to their corresponding views for handling compilation requests and language retrieval requests, respectively.\r\n\r\n\r\n3. **Third-Party Integrations**:\r\n    our project is integrated with third-party services or libraries for certain functionalities. For instance, the compilation feature in the `/compile` API is  relied on a third-party code execution engine, [Judge0](https://ce.judge0.com/) to compile and execute the submitted code securely. `/askai` and `/learnaskai` utilise [OpenAI](https://platform.openai.com/docs/introduction) API to give suggestions.\r\n\r\n\r\n## Backend Deployment\r\n\r\nThis project is deployed on PythonAnywhere at [adya2004.pythonanywhere.com](http://adya2004.pythonanywhere.com) \r\n\r\n\r\n## Contributing\r\n\r\nContributions to this project are welcome. If you find any issues or have suggestions for improvements, please open an issue or submit a pull request."
    },
    {
      "id": 43,
      "meta": {
        "type": "projects.ProjectsPage",
        "detail_url": "http://localhost/api/v2/pages/43/",
        "html_url": "http://localhost/projects/captionwiz-image-captioning-using-ml/",
        "slug": "captionwiz-image-captioning-using-ml",
        "show_in_menus": false,
        "seo_title": "",
        "search_description": "",
        "first_published_at": "2024-03-22T11:06:02.461282Z",
        "alias_of": null,
        "locale": "en"
      },
      "title": "CaptionWiz : Image Captioning using ML",
      "project_title": "CaptionWiz : Image Captioning using ML",
      "project_sig": "CIPHER",
      "project_slug": "captionwiz",
      "project_year": 2024,
      "project_authors": "Renukasakshi Patil, Avinash Kandala, Aman Revankar, Saket Ram Bandi, Abhyuday R.",
      "github_url": "https://github.com/AMANREVANKAR/captionwiz",
      "project_img_url": "https://tetragram.codered.cloud/media/images/captionwiz.max-800x600.png",
      "project_description": "Image captioning bridges the gap between computer vision and natural language processing. It tackles the complex task of analyzing an image's visual content and automatically generating captions that precisely describe what's seen. This project aims to develop a system that can interpret the objects, actions, and scene depicted in an image, and then translate that understanding into a natural language description, using NLP and Computer Vision techniques.",
      "project_tags": "Python, Pandas, Numpy, Streamlit, Tensorflow, Keras, expo24",
      "project_body": "# Captionwiz\r\n\r\n## Overview \r\nImage captioning is a challenging task in the field of computer vision and natural language processing. The goal of this project is to automatically generate captions that accurately describe the content of an image.\r\n\r\n## Model 1\r\n\r\n## Technology used\r\npython,pandas,numpy,tensorflow,keras,streamlit.\r\n\r\n\r\n## Features\r\n- ###  CNN Feature Extraction\r\n   Utilizes the Inception model to extract high-level features from input images.\r\n- ### LSTM Caption Generation:\r\n    Employs a Long Short-Term Memory (LSTM) network to generate captions based on the extracted image features.\r\n- ###  Streamlit Interface:\r\n    Hosted using Streamlit, providing a user-friendly interface for uploading images and viewing generated captions.\r\n- ### Pretrained Models:\r\n   Includes pretrained weights for both the CNN and LSTM models to facilitate quick deployment and usage.\r\n- ### Trained on Flickr Dataset:\r\n   The models are trained on the Flickr dataset, a widely used benchmark dataset for image captioning tasks.\r\n\r\n## Model Architecture\r\n\r\n#### The architecture of the image captioning model consists of two main components:\r\n\r\n- CNN (Convolutional Neural Network): Extracts high-level features from input images using the Inception model.\r\n\r\n- LSTM (Long Short-Term Memory): Generates descriptive captions based on the features extracted by the CNN.\r\n\r\n## Installation\r\n- Cloning the repo\r\n  git clone [repo link](https://github.com/AMANREVANKAR/captionwiz.git)\r\n- Activating the virtual environment\r\n  \r\n   run source env/bin/activate\r\n- Runining the streamlit file\r\n  \r\n   run streamlit run app.py\r\n\r\n## Working\r\n![image](https://github.com/AMANREVANKAR/captionwiz/assets/122635887/83e2b474-71a0-4f6d-bedb-710f6a6a517a)\r\n\r\n## Model 2\r\n\r\n## Model using CNN nad Transformer\r\n\r\n## Dependencies\r\n•⁠  ⁠Python 3.x\r\n•⁠  ⁠TensorFlow 2.x\r\n•⁠  ⁠Keras\r\n•⁠  ⁠NumPy\r\n•⁠  ⁠OpenCV\r\n\r\n \r\n## Dataset\r\nWe used the MS Coco - 2017 dataset for training and evaluation. It contains a large collection of images (around 6 hundeed thousand) along with corresponding captions.\r\n\r\n## Model Architecture\r\nWe employ a Transformer model for image captioning. The model consists of three main parts: A pretrained CNN (inceptionv3 is used for this project) for image extraction, the Transformer Encoder for processing the image features and the caption embeddings, and a Transformer Decoder for generating the output captions."
    }
  ]
}